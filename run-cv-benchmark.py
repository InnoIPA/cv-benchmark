#!/usr/bin/env python3
"""
ä½¿ç”¨éœæ…‹æ‰¹æ¬¡è™•ç† (Static Batching) çš„è¨ˆç®—æ©Ÿè¦–è¦ºåŸºæº–æ¸¬è©¦å·¥å…·ã€‚
æ­¤è…³æœ¬é€šéå°‡å¤šå€‹è¦–é »æµï¼ˆChannelsï¼‰çµ„åˆæˆä¸€å€‹æ‰¹æ¬¡ï¼Œ
ä½¿ç”¨å–®ä¸€æ¨¡å‹å¯¦ä¾‹é€²è¡Œæ¨è«–ï¼Œä»¥è©•ä¼°ååé‡å’Œå»¶é²ã€‚
"""
import argparse
import os
import sys
import threading
import time
import psutil
import json
from time import perf_counter
from typing import List, Tuple, Optional, Dict, Any
from datetime import datetime
import numpy as np
import cv2
import torch
from collections import deque
import queue
from torch.profiler import profile, record_function, ProfilerActivity
try:
    import pynvml
except ImportError:
    pynvml = None

try:
    from ultralytics import YOLO
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥ ultralytics: {e}")
    print("è«‹å®‰è£ ultralytics: pip install ultralytics")
    sys.exit(1)


class FixedChannelMetric:
    """å›ºå®šChannelæ€§èƒ½æŒ‡æ¨™æ”¶é›†å™¨"""
    
    def __init__(self, channel_id: int = 0) -> None:
        self.channel_id = channel_id
        self.lock = threading.Lock()
        
        # åŸºæœ¬æŒ‡æ¨™
        self.num_frames: int = 0
        self.total_proc_s: float = 0.0
        self.start_time = time.time()
        
        # æ€§èƒ½æ­·å²
        self.processing_times: List[float] = []
        self.fps_history: List[float] = []
        
        # æª¢æ¸¬æŒ‡æ¨™
        self.detection_counts: List[int] = []
        
        # æ¨¡å‹åˆ†é…ä¿¡æ¯
        self.assigned_model_id: int = -1
        self.model_shared: bool = False
        self.profiling_data: Dict[str, Any] = {}

    def update(self, proc_s: float, detections: int = 0) -> None:
        """æ›´æ–°æ€§èƒ½æŒ‡æ¨™"""
        with self.lock:
            self.num_frames += 1
            self.total_proc_s += float(proc_s)
            self.processing_times.append(proc_s)
            
            # è¨ˆç®—ç•¶å‰FPS
            current_fps = 1.0 / proc_s if proc_s > 0 else 0.0
            self.fps_history.append(current_fps)
            
            # æª¢æ¸¬æŒ‡æ¨™
            self.detection_counts.append(detections)

    def get_fps(self) -> float:
        """è¨ˆç®—å¯¦éš›FPS"""
        with self.lock:
            if self.num_frames <= 0:
                return 0.0
            elapsed = time.time() - self.start_time
            if elapsed <= 0:
                return 0.0
            return self.num_frames / elapsed

    def get_latency_ms(self) -> float:
        """è¨ˆç®—å¹³å‡å»¶é²ï¼ˆæ¯«ç§’ï¼‰"""
        with self.lock:
            if self.num_frames <= 0:
                return 0.0
            return (self.total_proc_s / self.num_frames) * 1000.0

    def get_throughput(self) -> float:
        """è¨ˆç®—ç¸½ååé‡ï¼ˆæ¯ç§’è™•ç†çš„å¹€æ•¸ï¼‰"""
        with self.lock:
            elapsed = time.time() - self.start_time
            if elapsed <= 0:
                return 0.0
            return self.num_frames / elapsed

    def get_avg_detections(self) -> float:
        """è¨ˆç®—å¹³å‡æª¢æ¸¬æ•¸é‡"""
        with self.lock:
            if not self.detection_counts:
                return 0.0
            return sum(self.detection_counts) / len(self.detection_counts)

class ResourceMonitor(threading.Thread):
    """è³‡æºç›£æ§å™¨ï¼Œç”¨æ–¼åœ¨æ¸¬è©¦æœŸé–“æ”¶é›†ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³"""
    def __init__(self, sample_interval: float = 1.0):
        super().__init__()
        self.daemon = True
        self._stop_event = threading.Event()
        self.sample_interval = sample_interval
        
        self.cpu_usage: List[float] = []
        self.memory_usage: List[float] = []
        self.gpu_usage: List[float] = []
        
        self._pynvml_initialized = False
        if pynvml:
            try:
                pynvml.nvmlInit()
                self._pynvml_initialized = True
            except pynvml.NVMLError:
                print("âš ï¸ ç„¡æ³•åˆå§‹åŒ– pynvmlï¼ŒGPU ä½¿ç”¨ç‡å°‡ä¸æœƒè¢«ç›£æ§ã€‚")

    def run(self) -> None:
        """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­å®šæœŸæ”¶é›†è³‡æºæ•¸æ“š"""
        while not self._stop_event.is_set():
            # æ”¶é›† CPU å’Œè¨˜æ†¶é«”ä½¿ç”¨ç‡
            self.cpu_usage.append(psutil.cpu_percent())
            self.memory_usage.append(psutil.virtual_memory().percent)
            
            # æ”¶é›† GPU ä½¿ç”¨ç‡
            gpu_percent = 0.0
            if self._pynvml_initialized:
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    gpu_percent = util.gpu
                except pynvml.NVMLError:
                    gpu_percent = 0.0 # å¦‚æœå‡ºéŒ¯ï¼Œå‰‡è¨˜éŒ„ç‚º0
            self.gpu_usage.append(gpu_percent)
            
            time.sleep(self.sample_interval)

    def stop(self) -> None:
        """åœæ­¢è³‡æºç›£æ§"""
        self._stop_event.set()
        if self._pynvml_initialized:
            try:
                pynvml.nvmlShutdown()
            except pynvml.NVMLError:
                pass

    def get_stats(self) -> Dict[str, Dict[str, float]]:
        """è¨ˆç®—ä¸¦è¿”å›è³‡æºä½¿ç”¨çš„çµ±è¨ˆæ•¸æ“š"""
        def _calculate(data: List[float]) -> Dict[str, float]:
            if not data:
                return {"average": 0.0, "min": 0.0, "max": 0.0}
            return {
                "average": float(np.mean(data)) if data else 0.0,
                "min": float(np.min(data)) if data else 0.0,
                "max": float(np.max(data)) if data else 0.0
            }

        return {
            "cpu": _calculate(self.cpu_usage),
            "memory": _calculate(self.memory_usage),
            "gpu": _calculate(self.gpu_usage)
        }


class FixedChannelBenchmark:
    """ä½¿ç”¨éœæ…‹æ‰¹æ¬¡è™•ç†çš„åŸºæº–æ¸¬è©¦ä¸»é¡"""
    
    def __init__(self,
                 model_name: str = 'yolov8n.pt',
                 device: str = 'auto',
                 img_size: int = 640,
                 conf_threshold: float = 0.25,
                 iou_threshold: float = 0.5):
        """åˆå§‹åŒ–éœæ…‹æ‰¹æ¬¡åŸºæº–æ¸¬è©¦å™¨"""
        self.model_name = model_name
        self.device = self._parse_device(device)
        self.img_size = img_size
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # ç¡¬é«”è¦æ ¼æª¢æ¸¬
        self.hardware_specs = self._detect_hardware_specs()
        
        print(f"ğŸš€ éœæ…‹æ‰¹æ¬¡åŸºæº–æ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   â€¢ æ¨¡å‹: {model_name}")
        print(f"   â€¢ è¨­å‚™: {self.device}")
        print(f"   â€¢ åœ–ç‰‡å°ºå¯¸: {img_size}x{img_size}")
        print(f"   â€¢ ç½®ä¿¡åº¦é–¾å€¼: {conf_threshold}")
        print(f"   â€¢ IoU é–¾å€¼: {iou_threshold}")
        print(f"   â€¢ ç¡¬é«”è¦æ ¼: {self.hardware_specs}")

    def _parse_device(self, device: str) -> str:
        """è§£æè¨­å‚™é…ç½®"""
        if device == 'auto':
            if torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        return device

    def _detect_hardware_specs(self) -> Dict[str, Any]:
        """æª¢æ¸¬ç¡¬é«”è¦æ ¼"""
        specs = {
            'cpu_cores': psutil.cpu_count(logical=False),
            'cpu_threads': psutil.cpu_count(logical=True),
            'total_memory_gb': psutil.virtual_memory().total / (1024**3),
            'cuda_available': torch.cuda.is_available(),
            'gpu_count': 0,
            'gpu_memory_gb': 0,
            'gpu_name': 'Unknown',
            'gpus': []
        }
        
        if torch.cuda.is_available():
            specs['gpu_count'] = torch.cuda.device_count()
            if specs['gpu_count'] > 0:
                # æª¢æ¸¬æ‰€æœ‰GPU
                for i in range(specs['gpu_count']):
                    gpu_props = torch.cuda.get_device_properties(i)
                    gpu_info = {
                        'id': i,
                        'name': gpu_props.name,
                        'memory_gb': gpu_props.total_memory / (1024**3),
                        'compute_capability': f"{gpu_props.major}.{gpu_props.minor}"
                    }
                    specs['gpus'].append(gpu_info)
                
                # ä¿æŒå‘å¾Œå…¼å®¹æ€§
                specs['gpu_memory_gb'] = specs['gpus'][0]['memory_gb']
                specs['gpu_name'] = specs['gpus'][0]['name']
        
        return specs


    def _test_model_loading(self, num_models: int) -> Tuple[bool, List[float], List[float]]:
        """æ¸¬è©¦è¼‰å…¥æŒ‡å®šæ•¸é‡çš„æ¨¡å‹"""
        print(f"ğŸ§ª æ¸¬è©¦è¼‰å…¥ {num_models} å€‹æ¨¡å‹...")
        
        models = []
        load_times = []
        memory_usage = []
        
        try:
            for i in range(num_models):
                print(f"   ğŸ”„ è¼‰å…¥æ¨¡å‹ {i+1}/{num_models}...")
                
                start_time = perf_counter()
                model = YOLO(self.model_name)
                
                if self.device != 'cpu':
                    model.to(self.device)
                
                load_time = perf_counter() - start_time
                load_times.append(load_time)
                
                # ç²å–è¨˜æ†¶é«”ä½¿ç”¨é‡
                if torch.cuda.is_available():
                    mem_usage = torch.cuda.memory_allocated() / (1024**3)
                    memory_usage.append(mem_usage)
                else:
                    memory_usage.append(0.0)
                
                models.append(model)
                print(f"   âœ… æ¨¡å‹ {i+1} è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.3f}ç§’")
            
            # æ¸…ç†æ¨¡å‹
            for model in models:
                del model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return True, load_times, memory_usage
            
        except Exception as e:
            print(f"   âŒ è¼‰å…¥å¤±æ•—: {e}")
            # æ¸…ç†å·²è¼‰å…¥çš„æ¨¡å‹
            for model in models:
                try:
                    del model
                except:
                    pass
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return False, load_times, memory_usage

    def _find_max_loadable_models(self, requested_channels: int) -> Tuple[int, Dict[str, Any]]:
        """å°‹æ‰¾æœ€å¤§å¯è¼‰å…¥çš„æ¨¡å‹æ•¸é‡ï¼ˆä¸Šé™ç‚ºChannelæ•¸é‡ï¼‰"""
        print(f"\nğŸ” å°‹æ‰¾æœ€å¤§å¯è¼‰å…¥æ¨¡å‹æ•¸é‡...")
        print(f"   â€¢ è«‹æ±‚Channelæ•¸: {requested_channels}")
        
        # å¾è«‹æ±‚çš„Channelæ•¸é–‹å§‹å¾€ä¸‹æ¸¬è©¦
        max_models = requested_channels
        print(f"   â€¢ èµ·å§‹æ¸¬è©¦æ¨¡å‹æ•¸: {max_models}")
        
        if max_models <= 0:
            print("   âŒ ç¡¬é«”è¦æ ¼ä¸è¶³ä»¥è¼‰å…¥ä»»ä½•æ¨¡å‹")
            return 0, {}
        
        # å¾ç†è«–æœ€å¤§å€¼é–‹å§‹ï¼Œé€æ­¥éæ¸›æ¸¬è©¦
        # å¾ç†è«–æœ€å¤§å€¼é–‹å§‹ï¼Œé€æ­¥éæ¸›æ¸¬è©¦
        for count in range(max_models, 0, -1):
            print(f"\n   ğŸ§ª æ¸¬è©¦ {count} å€‹æ¨¡å‹...")
            
            success, load_times, memory_usage = self._test_model_loading(count)
            
            if success:
                print(f"   âœ… æˆåŠŸè¼‰å…¥ {count} å€‹æ¨¡å‹")
                print(f"   â€¢ å¹³å‡è¼‰å…¥æ™‚é–“: {np.mean(load_times):.3f}ç§’")
                print(f"   â€¢ å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨: {np.mean(memory_usage):.2f} GB")
                
                return count, {
                    'load_times': load_times,
                    'memory_usage': memory_usage,
                    'avg_load_time': float(np.mean(load_times)) if load_times else 0.0,
                    'avg_memory_usage': float(np.mean(memory_usage)) if memory_usage else 0.0,
                    'total_memory_usage': float(np.sum(memory_usage)) if memory_usage else 0.0
                }
            else:
                print(f"   âŒ ç„¡æ³•è¼‰å…¥ {count} å€‹æ¨¡å‹")
        
        print("   âŒ ç„¡æ³•è¼‰å…¥ä»»ä½•æ¨¡å‹")
        return 0, {}

    def _create_model_instance(self, model_id: int) -> Tuple[YOLO, float, float]:
        """å‰µå»ºæ¨¡å‹å¯¦ä¾‹"""
        print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹å¯¦ä¾‹ {model_id}...")
        
        start_time = perf_counter()
        
        try:
            # å‰µå»ºæ–°çš„æ¨¡å‹å¯¦ä¾‹
            yolo_model = YOLO(self.model_name)
            
            if self.device != 'cpu':
                yolo_model.to(self.device)
            
            load_time = perf_counter() - start_time
            
            # ç²å–æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨é‡
            memory_usage = 0.0
            if torch.cuda.is_available():
                memory_usage = torch.cuda.memory_allocated() / (1024**3)  # GB
            
            print(f"âœ… æ¨¡å‹å¯¦ä¾‹ {model_id} è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.3f}ç§’")
            
            return yolo_model, load_time, memory_usage
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹å¯¦ä¾‹ {model_id} è¼‰å…¥å¤±æ•—: {e}")
            raise

    def predict_single_frame(self, model: YOLO, frame: np.ndarray) -> Tuple[List[Dict], float, float, float]:
        """
        [å®è§€æ¸¬è©¦ç”¨] å°å–®ä¸€å¹€é€²è¡Œé æ¸¬ï¼Œä¸¦å°‡æ¨è«–èˆ‡å¾Œè™•ç†åˆ†é–‹è¨ˆæ™‚ã€‚
        
        è¿”å›:
            detections (List[Dict]): æª¢æ¸¬çµæœ
            processing_time_s (float): ç¸½ç‰†ä¸Šæ™‚é–“ (ç§’)
            t_infer_s (float): ç´”æ¨è«– (model.predict) æ™‚é–“ (ç§’)
            t_post_s (float): ç´”å¾Œè™•ç† (cpu copy) æ™‚é–“ (ç§’)
        """
        t_wall_start = perf_counter()

        try:
            # --- 1. åŸ·è¡Œæ¨è«– (å–®ç¨è¨ˆæ™‚) ---
            t_infer_start = perf_counter()
            with torch.inference_mode():
                results = model.predict(
                    source=frame,
                    conf=self.conf_threshold,
                    iou=self.iou_threshold,
                    imgsz=self.img_size,
                    verbose=False,
                    save=False
                )
            t_infer_end = perf_counter()
            # --- æ¨è«–è¨ˆæ™‚çµæŸ ---

            # --- 2. CPU å¾Œè™•ç† (å–®ç¨è¨ˆæ™‚) ---
            t_post_start = perf_counter()
            detections = []
            if results:
                for r in results:
                    if r.boxes is not None:
                        # é€™è£¡ .cpu().numpy() æœƒå¼·åˆ¶ GPU åŒæ­¥
                        boxes = r.boxes.xyxy.cpu().numpy()
                        confidences = r.boxes.conf.cpu().numpy()
                        classes = r.boxes.cls.cpu().numpy().astype(int)
                        
                        for i in range(len(boxes)):
                            detection = {
                                'class_id': int(classes[i]),
                                'confidence': float(confidences[i]),
                                'bbox': boxes[i].tolist(),
                                'class_name': r.names[int(classes[i])]
                            }
                            detections.append(detection)
            t_post_end = perf_counter()
            # --- å¾Œè™•ç†è¨ˆæ™‚çµæŸ ---

            t_wall_end = perf_counter()
            processing_time_s = t_wall_end - t_wall_start
            
            # è¨ˆç®—æ–°çš„æŒ‡æ¨™
            t_infer_s = t_infer_end - t_infer_start
            t_post_s = t_post_end - t_post_start
            
            # è¿”å› 4 å€‹å€¼
            return detections, processing_time_s, t_infer_s, t_post_s
            
        except Exception as e:
            print(f"âš ï¸ é æ¸¬éŒ¯èª¤: {e}")
            # ç¢ºä¿è¿”å› 4 å€‹å€¼
            return [], 0.0, 0.0, 0.0

    def _profile_model_once(self, model: YOLO) -> Dict[str, float]:
        """
        [å¾®è§€å‰–æç”¨] åœ¨ä¸»åŸ·è¡Œç·’ä¸­å°å–®ä¸€æ¨¡å‹å¯¦ä¾‹é€²è¡Œè©³ç´°å‰–æã€‚
        é€™æœƒé ç†±ä¸¦é‹è¡Œå¤šæ¬¡æ¨è«–ï¼Œä»¥ç²å–ç©©å®šçš„ GPU é‹ç®—/I/O ç†è«–å€¼ã€‚
        
        è¿”å›:
            Dict[str, float]: åŒ…å« 'gpu_compute_avg_ms', 'gpu_io_avg_ms', 'cpu_post_proc_avg_ms' çš„å­—å…¸
        """
        print(f"   ğŸ”¬ [å¾®è§€å‰–æ] é–‹å§‹å° {self.model_name} é€²è¡Œå–®æ¨¡å‹ç†è«–å€¼åˆ†æ...")
        
        use_cuda = torch.cuda.is_available() and self.device != 'cpu'
        if not use_cuda:
            print("   âš ï¸ [å¾®è§€å‰–æ] æœªä½¿ç”¨ CUDAï¼Œè·³éè©³ç´°å‰–æã€‚")
            return {}

        # å‰µå»ºä¸€å€‹ç¬¦åˆ img_size çš„å‡ (dummy) åœ–åƒ
        dummy_frame = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)
        
        warmup_runs = 20
        profile_runs = 50
        
        results_compute: List[float] = []
        results_io: List[float] = []
        results_post_proc: List[float] = []

        try:
            # 1. é ç†± (Warm-up)
            print(f"   ğŸ”¬ [å¾®è§€å‰–æ] åŸ·è¡Œ {warmup_runs} æ¬¡é ç†±...")
            with torch.inference_mode():
                for _ in range(warmup_runs):
                    _ = model.predict(source=dummy_frame, verbose=False)
            
            # 2. å‰–æ (Profiling)
            print(f"   ğŸ”¬ [å¾®è§€å‰–æ] åŸ·è¡Œ {profile_runs} æ¬¡å‰–æ...")
            for _ in range(profile_runs):
                gpu_compute_s = 0.0
                gpu_io_s = 0.0
                
                with torch.inference_mode():
                    with profile(
                        activities=[ProfilerActivity.CUDA], # æˆ‘å€‘åªé—œå¿ƒ CUDA äº‹ä»¶
                        record_shapes=False,
                        with_stack=False
                    ) as prof:
                        results = model.predict(
                            source=dummy_frame,
                            conf=self.conf_threshold,
                            iou=self.iou_threshold,
                            imgsz=self.img_size,
                            verbose=False,
                            save=False
                        )
                
                # æå– Profiler æ•¸æ“š
                for event in prof.events():
                    if "memcpy" in event.name.lower():
                        gpu_io_s += event.cuda_time_total / 1_000_000.0  # us -> s
                    elif "kernel" in event.name.lower():
                        gpu_compute_s += event.cuda_time_total / 1_000_000.0 # us -> s
                
                # æ¸¬é‡ CPU å¾Œè™•ç†
                t_post_start = perf_counter()
                if results:
                    for r in results:
                        _ = r.boxes.xyxy.cpu().numpy() # æ¨¡æ“¬å¾Œè™•ç†
                cpu_post_proc_s = perf_counter() - t_post_start
                
                results_compute.append(gpu_compute_s)
                results_io.append(gpu_io_s)
                results_post_proc.append(cpu_post_proc_s)
            
            # 3. è¨ˆç®—å¹³å‡å€¼ä¸¦è½‰æ›ç‚ºæ¯«ç§’ (ms)
            avg_compute_ms = (sum(results_compute) / len(results_compute)) * 1000
            avg_io_ms = (sum(results_io) / len(results_io)) * 1000
            avg_post_proc_ms = (sum(results_post_proc) / len(results_post_proc)) * 1000
            
            result_dict = {
                "micro_gpu_compute_avg_ms": avg_compute_ms,
                "micro_gpu_io_avg_ms": avg_io_ms,
                "micro_cpu_post_proc_avg_ms": avg_post_proc_ms,
                "micro_total_avg_ms": avg_compute_ms + avg_io_ms + avg_post_proc_ms
            }
            print(f"   âœ… [å¾®è§€å‰–æ] å®Œæˆ: {result_dict}")
            return result_dict
            
        except Exception as e:
            print(f"   âŒ [å¾®è§€å‰–æ] å¤±æ•—: {e}")
            return {}

    def benchmark_video_fixed_channels(self,
                                      video_path: str,
                                      duration_seconds: int = 60,
                                      requested_channels: int = 1,
                                      output_file: Optional[str] = None,
                                      save_preview: bool = False) -> Dict[str, Any]:
        """
        ä½¿ç”¨éœæ…‹æ‰¹æ¬¡è™•ç†åŸ·è¡Œè¦–é »åŸºæº–æ¸¬è©¦ã€‚
        `requested_channels` åƒæ•¸å°‡è¢«ç”¨ä½œæ‰¹æ¬¡å¤§å° (batch size)ã€‚
        """
        # è¨˜éŒ„æ¸¬è©¦é–‹å§‹æ™‚é–“
        test_start_time = time.time()
        
        print(f"ğŸ¬ é–‹å§‹éœæ…‹æ‰¹æ¬¡åŸºæº–æ¸¬è©¦")
        print(f"   â€¢ è¦–é »: {video_path}")
        print(f"   â€¢ æŒçºŒæ™‚é–“: {duration_seconds}ç§’")
        print(f"   â€¢ æ‰¹æ¬¡å¤§å° (Channels): {requested_channels}")

        # åœ¨æ‰¹æ¬¡æ¨¡å¼ä¸‹ï¼Œæˆ‘å€‘ç¸½æ˜¯ä½¿ç”¨ 1 å€‹æ¨¡å‹
        max_models = 1
        
        # é©—è­‰è¦–é »æ–‡ä»¶
        if not os.path.isfile(video_path):
            raise FileNotFoundError(f"è¦–é »æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # ç²å–è¦–é »ä¿¡æ¯
        video_info = self._get_video_info(video_path)
        print(f"ğŸ“¹ è¦–é »ä¿¡æ¯: {video_info['width']}x{video_info['height']}, {video_info['fps']:.2f} FPS")
        
        # æ¸¬è©¦è¼‰å…¥ 1 å€‹æ¨¡å‹
        success, load_times, memory_usage = self._test_model_loading(max_models)
        if not success:
            print(f"âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼Œæ¸¬è©¦çµ‚æ­¢")
            return {}
        
        load_info = {
            'load_times': load_times,
            'memory_usage': memory_usage,
            'avg_load_time': float(np.mean(load_times)) if load_times else 0.0,
            'avg_memory_usage': float(np.mean(memory_usage)) if memory_usage else 0.0,
            'total_memory_usage': float(np.sum(memory_usage)) if memory_usage else 0.0
        }

        print(f"\nğŸ¯ åŸ·è¡Œç­–ç•¥:")
        print(f"   â€¢ æ‰¹æ¬¡å¤§å°: {requested_channels}")
        print(f"   â€¢ è¼‰å…¥æ¨¡å‹æ•¸: {max_models}")
        print(f"   â€¢ é…ç½®æ–¹å¼: éœæ…‹æ‰¹æ¬¡è™•ç†")
        
        channels_per_model = requested_channels / max_models
        
        # åˆå§‹åŒ–ChannelæŒ‡æ¨™æ”¶é›†å™¨
        channel_metrics = [FixedChannelMetric(i) for i in range(requested_channels)]
        
        # è¼‰å…¥æ¨¡å‹å¯¦ä¾‹
        print(f"\nğŸ”„ è¼‰å…¥ {max_models} å€‹æ¨¡å‹å¯¦ä¾‹...")
        models = []
        model_load_times = []
        model_memory_usage = []
        
        for i in range(max_models):
            model, load_time, memory_usage = self._create_model_instance(i)
            models.append(model)
            model_load_times.append(load_time)
            model_memory_usage.append(memory_usage)
        
        print(f"âœ… æ‰€æœ‰æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œç¸½è€—æ™‚: {sum(model_load_times):.3f}ç§’")
        
        # --- åŸ·è¡Œä¸€æ¬¡å¾®è§€å‰–æ (ä»»å‹™ A) ---
        micro_profiling_results = {}
        if models:
            micro_profiling_results = self._profile_model_once(models[0])
        else:
            print("   âš ï¸ æ²’æœ‰è¼‰å…¥ä»»ä½•æ¨¡å‹ï¼Œè·³éå¾®è§€å‰–æã€‚")
            
        # å‰µå»ºChannelåˆ†é…æ˜ å°„
        channel_to_model = {}
        for channel_id in range(requested_channels):
            if max_models >= requested_channels:
                # ç†æƒ³é…ç½®ï¼šæ¯å€‹Channeléƒ½æœ‰å°ˆå±¬æ¨¡å‹
                model_id = channel_id
                channel_metrics[channel_id].model_shared = False
            else:
                # æ¨¡å‹å…±äº«ï¼šå¤šå€‹Channelå…±äº«æ¨¡å‹
                model_id = channel_id % max_models
                channel_metrics[channel_id].model_shared = True
            
            channel_to_model[channel_id] = model_id
            channel_metrics[channel_id].assigned_model_id = model_id
        
        print(f"ğŸ“‹ Channelåˆ†é…æ˜ å°„:")
        for channel_id in range(requested_channels):
            model_id = channel_to_model[channel_id]
            print(f"   â€¢ Channel {channel_id} â†’ Model {model_id}")
        
        # åˆå§‹åŒ–ä¸¦å•Ÿå‹•è³‡æºç›£æ§å™¨
        resource_monitor = ResourceMonitor()
        resource_monitor.start()

        # --- ğŸ‘‡ ç°¡åŒ–ç‚ºåƒ…å•Ÿå‹•æ‰¹æ¬¡å·¥ä½œç·šç¨‹çš„é‚è¼¯ --- ğŸ‘‡
        threads = []
        stop_ts = time.time() + duration_seconds
        
        print(f"ğŸš€ å•Ÿå‹• 1 å€‹æ‰¹æ¬¡å·¥ä½œç·šç¨‹ (Batch Size = {requested_channels}) ...")
        
        # 1. æˆ‘å€‘åªä½¿ç”¨ 1 å€‹æ¨¡å‹
        batch_model = models[0]
        
        # 2. æˆ‘å€‘åªå•Ÿå‹• 1 å€‹å·¥ä½œç·šç¨‹
        # é€™å€‹ç·šç¨‹å°‡è™•ç† *æ‰€æœ‰* channels
        batch_thread = threading.Thread(
            target=self._fixed_channel_worker_batch,
            args=(
                video_path,
                stop_ts,
                channel_metrics, # å‚³é *æ‰€æœ‰* metrics
                batch_model,
                requested_channels, # å‘Šè¨´å®ƒæ‰¹æ¬¡å¤§å° (N)
                save_preview # å‚³éé è¦½æ¨™èªŒ
            ),
            daemon=True
        )
        batch_thread.start()
        threads = [batch_thread] # åªæœ‰ä¸€å€‹ç·šç¨‹
        # --- ğŸ‘† æ›¿æ›çµæŸ --- ğŸ‘†
        
        print("âœ… é–‹å§‹å›ºå®šChannelæ€§èƒ½ç›£æ§\n")
        
        # å®šæœŸå ±å‘Š
        self._fixed_channel_monitor_progress(channel_metrics, stop_ts)

        # ç­‰å¾…æ‰€æœ‰ç·šç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # åœæ­¢è³‡æºç›£æ§ä¸¦ç²å–æ•¸æ“š
        resource_monitor.stop()
        resource_monitor.join()
        resource_stats = resource_monitor.get_stats()
        
        # æ¸…ç†æ¨¡å‹
        for model in models:
            del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
        test_end_time = time.time()
        total_execution_time = test_end_time - test_start_time
        
        # --- ğŸ‘‡ ä¿®æ”¹ config å­—å…¸ --- ğŸ‘‡
        config = {
            'model': self.model_name,
            'video': video_path,
            'requested_channels': requested_channels,
            'actual_models': max_models,
            'channels_per_model': channels_per_model,
            'fixed_models': 1, # åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹æ•¸å§‹çµ‚ç‚º1
            'img_size': self.img_size,
            'video_resolution': f"{video_info['width']}x{video_info['height']}",
            'video_fps': video_info['fps'],
            'conf': self.conf_threshold,
            'iou': self.iou_threshold,
            'seconds': duration_seconds,
            'device': self.device,
            'model_load_time': sum(model_load_times),
            'total_execution_time': total_execution_time,
            
            # --- æ›¿æ› 'architecture' ---
            'architecture': 'static_batching',
            
            'hardware_specs': self.hardware_specs,
            'load_info': load_info,
            'channel_allocation': channel_to_model
        }
        # --- ğŸ‘† ---
        
        # å°‡å¾®è§€å‰–æçµæœ (micro_profiling_results) å‚³éçµ¦å ±å‘Šç”Ÿæˆå™¨
        report = self._generate_fixed_channel_report(
            channel_metrics, config, resource_stats, micro_profiling_results
        )
        # --- ğŸ‘† ä¿®æ­£çµæŸ --- ğŸ‘†
        
        self._print_fixed_channel_report(report)
        
        # è‡ªå‹•ç”Ÿæˆå ±å‘Šæª”æ¡ˆåï¼ˆå¦‚æœæ²’æœ‰æŒ‡å®šï¼‰
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = os.path.splitext(os.path.basename(self.model_name))[0]
            output_file = f"reports/cv_benchmark_{model_name}_st_{requested_channels}ch_{timestamp}.json"
        
        # ç¢ºä¿ reports ç›®éŒ„å­˜åœ¨
        os.makedirs("reports", exist_ok=True)
        
        # ä¿å­˜å ±å‘Š
        self._save_report(report, output_file)
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {output_file}")
        
        return report

    def run_auto_optimization(self, args: argparse.Namespace) -> Dict[str, Any]:
        """
        è‡ªå‹•å„ªåŒ–ä¸»å‡½æ•¸ï¼Œè¿­ä»£ä¸åŒçš„æ‰¹æ¬¡å¤§å°ï¼ˆChannelsï¼‰ï¼Œä»¥æ‰¾åˆ°æœ€ä½³ååé‡ã€‚
        (å·²ä¿®æ”¹ç‚ºæ¸¬è©¦ 2 çš„å†ªæ¬¡æ–¹)
        """
        optimization_start_time = time.time()
        print(f"ğŸš€ é–‹å§‹è‡ªå‹•å„ªåŒ–æ‰¹æ¬¡å¤§å°æ¸¬è©¦")
        print(f"   â€¢ è¦–é »: {args.video}")
        print(f"   â€¢ æŒçºŒæ™‚é–“: {args.seconds}ç§’")
        print(f"   â€¢ æœ€å¤§æ¸¬è©¦æ‰¹æ¬¡å¤§å°: {args.channels}")
        video_info = self._get_video_info(args.video)
        print(f"ğŸ“¹ è¦–é »ä¿¡æ¯: {video_info['width']}x{video_info['height']}, {video_info['fps']:.2f} FPS")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name_base = os.path.splitext(os.path.basename(self.model_name))[0]
        output_dir = getattr(args, 'output_dir', 'reports')
        report_dir_name = f"cv_optimization_{model_name_base}_{args.channels}ch_{timestamp}"
        report_dir = os.path.join(output_dir, report_dir_name)
        os.makedirs(report_dir, exist_ok=True)
        print(f"ğŸ“‚ å ±å‘Šå°‡å„²å­˜æ–¼: {report_dir}")
        
        test_results = []
        
        # æ–°é‚è¼¯ï¼šåªæ¸¬è©¦ 2 çš„å†ªæ¬¡æ–¹ï¼Œä»¥æ‰¾åˆ°æœ€ä½³æ•ˆèƒ½é»
        test_configs_set = set()
        batch_size = 1
        while batch_size <= args.channels:
            test_configs_set.add(batch_size)
            batch_size *= 2
        
        # ç¢ºä¿ã€Œæœ€å¤§å€¼ã€ (N) ç¸½æ˜¯è¢«æ¸¬è©¦åˆ°ï¼Œä»¥é˜²å®ƒæ˜¯æœ€ä½³è§£
        test_configs_set.add(args.channels)
        
        test_configs = sorted(list(test_configs_set))

        print(f"\nğŸ” [å„ªåŒ–] å°‡åŸ·è¡Œ {len(test_configs)} æ¬¡æ¸¬è©¦ï¼Œæ¸¬è©¦çš„æ‰¹æ¬¡å¤§å°ç‚º: {test_configs}")
        
        for i, batch_size in enumerate(test_configs, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ§ª æ¸¬è©¦ {i}/{len(test_configs)}: æ‰¹æ¬¡å¤§å° = {batch_size}")
            print(f"{'='*60}")
            
            try:
                intermediate_report_name = f"benchmark_batch_{batch_size}.json"
                intermediate_output_file = os.path.join(report_dir, intermediate_report_name)
                
                result = self.benchmark_video_fixed_channels(
                    video_path=args.video,
                    duration_seconds=args.seconds,
                    requested_channels=batch_size,
                    output_file=intermediate_output_file,
                    save_preview=getattr(args, 'save_preview', False)
                )
                
                if result and 'performance_metrics' in result:
                    perf = result['performance_metrics']
                    
                    # æ ¹æ“šæ–°çš„ JSON çµæ§‹æå–é—œéµæŒ‡æ¨™
                    avg_fps = perf.get('fps', {}).get('average')
                    total_fps = perf.get('fps', {}).get('total')
                    avg_latency = perf.get('latency_ms', {}).get('average')

                    if avg_fps is None or total_fps is None or avg_latency is None:
                        print(f"âŒ æ¸¬è©¦çµæœç¼ºå°‘é—œéµæŒ‡æ¨™: æ‰¹æ¬¡å¤§å° = {batch_size}")
                        continue

                    model_count = 1
                    channels_per_model = batch_size / model_count

                    efficiency_score = self._calculate_efficiency_score(
                        avg_fps, total_fps, avg_latency,
                        batch_size, model_count, channels_per_model
                    )
                    
                    summary = {
                        'batch_size': batch_size,
                        'avg_fps': avg_fps,
                        'total_fps': total_fps,
                        'avg_latency': avg_latency,
                        'efficiency_score': efficiency_score,
                        'resource_usage': perf.get('resource_usage', {}),
                        'report_file': intermediate_report_name
                    }
                    
                    if 'profiling_details' in result:
                        summary['profiling_summary'] = result['profiling_details']
                    
                    test_results.append(summary)
                    
                    print(f"âœ… æ¸¬è©¦å®Œæˆ: æ‰¹æ¬¡å¤§å° = {batch_size}")
                    print(f"   â€¢ å¹³å‡FPS: {avg_fps:.2f}")
                    print(f"   â€¢ ç¸½FPS (ååé‡): {total_fps:.2f}")
                    print(f"   â€¢ å¹³å‡å»¶é²: {avg_latency:.2f}ms")
                    print(f"   â€¢ æ•ˆç‡åˆ†æ•¸: {efficiency_score:.2f}")
                    
                else:
                    print(f"âŒ æ¸¬è©¦å¤±æ•—: æ‰¹æ¬¡å¤§å° = {batch_size}")
                    
            except Exception as e:
                print(f"âŒ æ¸¬è©¦éŒ¯èª¤: æ‰¹æ¬¡å¤§å° = {batch_size} - {e}")
                continue
        
        if not test_results:
            print("âŒ æ‰€æœ‰æ¸¬è©¦éƒ½å¤±æ•—äº†ï¼Œç„¡æ³•ç”Ÿæˆå„ªåŒ–å ±å‘Š")
            return {}
        
        best_config = max(test_results, key=lambda x: x['total_fps'])
        
        optimization_report = self._generate_optimization_report(
            test_results, best_config, video_info, args.channels
        )
        
        self._print_optimization_report(optimization_report)
        
        final_report_name = "optimization_report.json"
        final_output_file = os.path.join(report_dir, final_report_name)
        
        self._save_report(optimization_report, final_output_file)
        print(f"\nğŸ“„ å„ªåŒ–å ±å‘Šå·²ä¿å­˜è‡³: {final_output_file}")
        
        return optimization_report

    def _calculate_efficiency_score(self, avg_fps: float, total_fps: float,
                                  avg_latency: float, batch_size: int,
                                  model_count: int, channels_per_model: float) -> float:
        """è¨ˆç®—æ•ˆç‡åˆ†æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼Œå°ˆæ³¨æ–¼ååé‡å’Œå»¶é²ï¼‰"""
        # æ¬Šé‡é…ç½®
        throughput_weight = 0.6  # ç¸½ååé‡æ¬Šé‡
        latency_weight = 0.4   # å»¶é²æ¬Šé‡
        
        # ååé‡åˆ†æ•¸ (0-100)ï¼Œä»¥ä¸€å€‹åƒè€ƒå€¼ï¼ˆå¦‚ 100 FPSï¼‰ç‚ºåŸºæº–
        throughput_score = min(100, (total_fps / 100) * 100)
        
        # å»¶é²åˆ†æ•¸ (0-100)ï¼Œå»¶é²è¶Šä½åˆ†æ•¸è¶Šé«˜
        latency_score = max(0, 100 - (avg_latency / 100) * 100)  # ä»¥100msç‚ºåŸºæº–
        
        # è¨ˆç®—ç¸½åˆ†
        total_score = (throughput_score * throughput_weight +
                       latency_score * latency_weight)
        
        return total_score

    def _generate_optimization_report(self, results: List[Dict], best_config: Dict,
                                    video_info: Dict, max_batch_size: int) -> Dict:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "sdk_info": {
                "name": "Auto-Optimization Batch Benchmark",
                "version": "1.1.0",
                "framework": "PyTorch + Ultralytics YOLO"
            },
            "test_configuration": {
                "video": video_info.get('path', 'N/A'),
                "max_batch_size_tested": max_batch_size,
                "model": self.model_name,
                "device": self.device,
                "img_size": self.img_size
            },
            "test_results": results,
            "best_configuration": best_config,
            "optimization_summary": {
                "total_tests": len(results),
                "best_batch_size": best_config.get('batch_size', 0),
                "best_total_fps": best_config.get('total_fps', 0),
                "latency_at_best_fps": best_config.get('avg_latency', 0),
                "recommendation": f"ç‚ºç²å¾—æœ€é«˜ååé‡ï¼Œå»ºè­°ä½¿ç”¨æ‰¹æ¬¡å¤§å°ç‚º {best_config.get('batch_size', 0)}ã€‚"
            }
        }
        
        return report

    def _print_optimization_report(self, report: Dict):
        """é¡¯ç¤ºå„ªåŒ–å ±å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ è‡ªå‹•å„ªåŒ–æ‰¹æ¬¡å¤§å°çµæœå ±å‘Š")
        print(f"{'='*80}")
        
        # æœ€ä½³é…ç½®
        summary = report['optimization_summary']
        test_config = report['test_configuration']
        
        print(f"\nğŸ† æœ€ä½³é…ç½® (åŸºæ–¼æœ€é«˜ç¸½ååé‡):")
        print(f"  â€¢ æœ€ä½³æ‰¹æ¬¡å¤§å°: {summary['best_batch_size']}")
        print(f"  â€¢ æœ€é«˜ç¸½FPS (ååé‡): {summary['best_total_fps']:.2f}")
        print(f"  â€¢ åœ¨æ­¤é…ç½®ä¸‹çš„å¹³å‡å»¶é²: {summary['latency_at_best_fps']:.2f}ms")
        print(f"  â€¢ å»ºè­°: {summary['recommendation']}")
        
        # æ‰€æœ‰æ¸¬è©¦çµæœ
        print(f"\nğŸ“Š æ‰€æœ‰æ¸¬è©¦çµæœ:")
        print(f"{'æ‰¹æ¬¡å¤§å°':<10} {'ç¸½FPS':<12} {'å¹³å‡å»¶é²(ms)':<15} {'Avg CPU(%)':<12} {'Avg GPU(%)':<12}")
        print(f"{'-'*75}")
        
        for result in report['test_results']:
            resource_usage = result.get('resource_usage', {})
            avg_cpu = resource_usage.get('cpu', {}).get('average', 0.0)
            avg_gpu = resource_usage.get('gpu', {}).get('average', 0.0)
            
            print(f"{result['batch_size']:<10} {result['total_fps']:<12.2f} {result['avg_latency']:<15.2f} "
                  f"{avg_cpu:<12.1f} {avg_gpu:<12.1f}")
        
        # ä½¿ç”¨å»ºè­°
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print(f"  â€¢ æœ€ä½³æŒ‡ä»¤: python {os.path.basename(__file__)} --video {test_config['video']} --model {self.model_name} -n {summary['best_batch_size']} -t 60")
        print(f"  â€¢ é æœŸç¸½ååé‡: {summary['best_total_fps']:.1f} frames/sec")

    def _fixed_channel_worker_batch(self,
                                  video_path: str,
                                  stop_ts: float,
                                  all_metrics: List[FixedChannelMetric],
                                  model: YOLO,
                                  batch_size: int,
                                  save_preview: bool = False):
        """
        [æ‰¹æ¬¡è™•ç† (Batching) å·¥ä½œè€…]
        æ­¤å–®ä¸€ç·šç¨‹å·¥ä½œè€…æœƒï¼š
        1. é–‹å•Ÿ 'batch_size' å€‹ video capturesã€‚
        2. å¾æ¯å€‹ capture è®€å– 1 å¹€ï¼Œçµ„æˆä¸€å€‹ batchã€‚
        3. ä¸€æ¬¡æ€§å‘¼å« model.predict(batch)ã€‚
        4. å°‡çµæœåˆ†ç™¼å› 'all_metrics' åˆ—è¡¨ã€‚
        """
        print(f"ğŸ”„ æ‰¹æ¬¡å·¥ä½œè€… [Batch size={batch_size}] é–‹å§‹å·¥ä½œ (ä½¿ç”¨ Model 0)")
        
        # 1. ç‚º 'N' å€‹ Channel é–‹å•Ÿ 'N' å€‹ video captures
        caps = []
        for i in range(batch_size):
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"[Batch Worker] âŒ ç„¡æ³•æ‰“é–‹ Channel {i} çš„è¦–é »")
                return
            caps.append(cap)
        
        # æº–å‚™æ”¶é›†å‰–ææ•¸æ“š
        read_times = []
        batch_predict_wall_times = []
        batch_predict_infer_times = []
        batch_predict_post_times = []
        
        frame_batch = [None] * batch_size # é åˆ†é…åˆ—è¡¨
        valid_frame_indices = []

        # --- ğŸ‘‡ [é—œéµä¿®æ”¹ 1] æ–°å¢ä¸€å€‹æ¨™è¨˜ï¼Œç¢ºä¿æˆ‘å€‘åªå„²å­˜ç¬¬ä¸€æ‰¹å½±åƒ ---
        has_saved_preview_images = False
        preview_dir = "preview_outputs" # å„²å­˜å½±åƒçš„è³‡æ–™å¤¾
        if save_preview:
            os.makedirs(preview_dir, exist_ok=True)
        # --- ğŸ‘† ---

        try:
            while time.time() < stop_ts:
                
                # --- [A] ä»»å‹™ Aï¼šå»ºç«‹æ‰¹æ¬¡ (Build Batch) ---
                t_read_start = perf_counter()
                
                valid_frame_indices.clear()
                
                for i in range(batch_size):
                    ret, frame = caps[i].read()
                    if not ret:
                        caps[i].set(cv2.CAP_PROP_POS_FRAMES, 0) # é‡æ’­
                        ret, frame = caps[i].read()
                    
                    if ret:
                        # --- ğŸ‘‡ [é—œéµä¿®æ”¹ 2] åœ¨é€å…¥æ¨¡å‹å‰ï¼Œå…ˆç•«ä¸Š ID ---
                        cv2.putText(
                            frame,
                            f"INPUT CHANNEL {i}",
                            (50, 50), # åº§æ¨™
                            cv2.FONT_HERSHEY_SIMPLEX,
                            2, # å­—é«”å¤§å°
                            (0, 0, 255), # é¡è‰² (BGR ç´…è‰²)
                            3 # ç·šæ¢ç²—ç´°
                        )
                        # --- ğŸ‘† ---
                        
                        frame_batch[i] = frame # æ”¾å…¥æ‰¹æ¬¡ (å·²ç•«ä¸Šæ–‡å­—çš„ Numpy array)
                        valid_frame_indices.append(i)
                    else:
                        frame_batch[i] = None
                    
                t_read_end = perf_counter()
                if not valid_frame_indices:
                    continue
                
                current_batch = [frame_batch[i] for i in valid_frame_indices]
                read_times.append(t_read_end - t_read_start)
                
                # --- [B] ä»»å‹™ Bï¼šæ¨è«–æ‰¹æ¬¡ (Inference) ---
                t_wall_start = perf_counter()
                
                t_infer_start = perf_counter()
                with torch.inference_mode():
                    results_list = model.predict(
                        source=current_batch,
                        conf=self.conf_threshold,
                        iou=self.iou_threshold,
                        verbose=False,
                        save=False
                    )
                t_infer_end = perf_counter()

                # --- ğŸ‘‡ [é—œéµä¿®æ”¹ 3] å¦‚æœå•Ÿç”¨ï¼Œå„²å­˜æˆ‘å€‘ç•«å¥½çš„é è¦½å½±åƒ ---
                if save_preview and not has_saved_preview_images and results_list:
                    print(f"ğŸ“¸ [Preview] æ­£åœ¨å„²å­˜ç¬¬ä¸€å€‹æ‰¹æ¬¡çš„ {len(results_list)} å¼µå·²æ¨™è¨»å½±åƒ...")
                    
                    for idx, result_obj in enumerate(results_list):
                        output_frame_with_boxes = result_obj.plot()
                        original_channel_id = valid_frame_indices[idx]
                        save_path = os.path.join(preview_dir, f"output_channel_{original_channel_id}_(batch_index_{idx}).jpg")
                        cv2.imwrite(save_path, output_frame_with_boxes)
                        
                    print(f"âœ… [Preview] å½±åƒå·²å„²å­˜è‡³ {preview_dir} è³‡æ–™å¤¾")
                    has_saved_preview_images = True # æ¨™è¨˜ç‚º trueï¼Œä¹‹å¾Œä¸å†å„²å­˜
                # --- ğŸ‘† ---
                
                # 2. å¾Œè™•ç† (å–®ç¨è¨ˆæ™‚)
                t_post_start = perf_counter()
                # 'results_list' æ˜¯ä¸€å€‹åŒ…å« 'batch_size' å€‹çµæœçš„åˆ—è¡¨
                all_detections = []
                for r in results_list: # éæ­·æ‰¹æ¬¡ä¸­çš„æ¯å€‹çµæœ
                    detections = []
                    if r.boxes is not None:
                        boxes = r.boxes.xyxy.cpu().numpy()
                        confidences = r.boxes.conf.cpu().numpy()
                        classes = r.boxes.cls.cpu().numpy().astype(int)
                        
                        for i in range(len(boxes)):
                            det = {
                                'class_id': int(classes[i]),
                                'confidence': float(confidences[i]),
                                'bbox': boxes[i].tolist(),
                                'class_name': r.names[int(classes[i])]
                            }
                            detections.append(det)
                    all_detections.append(detections)
                t_post_end = perf_counter()
                
                t_wall_end = perf_counter()
                
                # --- [C] å„²å­˜æ‰¹æ¬¡æŒ‡æ¨™ ---
                wall_s = t_wall_end - t_wall_start
                infer_s = t_infer_end - t_infer_start
                post_s = t_post_end - t_post_start
                
                batch_predict_wall_times.append(wall_s)
                batch_predict_infer_times.append(infer_s)
                batch_predict_post_times.append(post_s)
                
                # --- [D] æ›´æ–° *æ‰€æœ‰* Channel çš„æŒ‡æ¨™ ---
                # é€™æ˜¯é—œéµï¼šæˆ‘å€‘ä½¿ç”¨ã€Œæ”¤æ (Amortized)ã€å»¶é²
                amortized_latency_s = wall_s / len(valid_frame_indices) if valid_frame_indices else 0
                
                for i in valid_frame_indices:
                    metric = all_metrics[i]
                    # all_detections çš„ç´¢å¼•ç¾åœ¨æ‡‰è©²å°æ‡‰åˆ° results_list çš„ç´¢å¼•
                    # è€Œ results_list çš„ç´¢å¼•å°æ‡‰åˆ° valid_frame_indices
                    # æ‰€ä»¥æˆ‘å€‘éœ€è¦æ‰¾åˆ° valid_frame_indices ä¸­ i çš„ä½ç½®
                    try:
                        result_idx = valid_frame_indices.index(i)
                        num_dets = len(all_detections[result_idx])
                        metric.update(amortized_latency_s, num_dets)
                    except (ValueError, IndexError):
                        # å¦‚æœç™¼ç”ŸéŒ¯èª¤ï¼Œè·³éæ­¤å¹€çš„æ›´æ–°
                        pass
                    
        except Exception as e:
            print(f"[Batch Worker] å·¥ä½œç·šç¨‹éŒ¯èª¤: {e}")
        finally:
            for cap in caps:
                cap.release()
            
            # å›å‚³ *æ‰¹æ¬¡* çš„å‰–ææ•¸æ“š
            # æˆ‘å€‘åªæ›´æ–° channel_0 çš„ metric.profiling_data
            # (é€™æ˜¯ä¸€å€‹ç°¡åŒ–ï¼Œå› ç‚ºæ‰€æœ‰ channel å…±äº«é€™å€‹æ•¸æ“š)
            batch_profiling_data = {
                'batch_read_total_times': read_times, # é€™æ˜¯ N å¹€çš„ç¸½æ™‚é–“
                'batch_predict_wall_times': batch_predict_wall_times,
                'batch_predict_infer_times': batch_predict_infer_times,
                'batch_predict_post_times': batch_predict_post_times,
                'batch_size': batch_size
            }
            if all_metrics:
                all_metrics[0].profiling_data = batch_profiling_data

    def _get_video_info(self, video_path: str) -> Dict[str, Any]:
        """ç²å–è¦–é »ä¿¡æ¯"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return {'width': 0, 'height': 0, 'fps': 0, 'frame_count': 0}
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        cap.release()
        
        return {
            'width': width,
            'height': height,
            'fps': fps,
            'frame_count': frame_count
        }

    def _fixed_channel_monitor_progress(self, metrics: List[FixedChannelMetric], stop_ts: float):
        """å›ºå®šChannelé€²åº¦ç›£æ§"""
        last_emit_s = -1
        
        while time.time() < stop_ts:
            elapsed = int(stop_ts - time.time())
            now_s = int(time.time())
            
            if last_emit_s == -1 or (now_s - last_emit_s) >= 3:
                last_emit_s = now_s
                
                for metric in metrics:
                    fps = metric.get_fps()
                    latency = metric.get_latency_ms()
                    throughput = metric.get_throughput()
                    detections = metric.get_avg_detections()
                    
                    model_info = f"Model {metric.assigned_model_id}"
                    
                    print(
                        f"Channel {metric.channel_id} ({model_info}): fps={fps:.3f}, latency={latency:.2f}ms, "
                        f"detections={detections:.1f}"
                    )
                print("")
            
            time.sleep(0.5)

    def _generate_fixed_channel_report(self, 
                                     metrics: List[FixedChannelMetric], 
                                     config: Dict, 
                                     resource_stats: Dict, 
                                     micro_profiling: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆå›ºå®šChannelå ±å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            # ... (sdk_info, configuration, summary å…§å®¹ä¸è®Š) ...
            "sdk_info": {
                "name": "Static Batching Benchmark",
                "version": "1.1.0",
                "framework": "PyTorch + Ultralytics YOLO"
            },
            "configuration": config,
            "summary": {
                "total_channels": len(metrics),
                "total_models": config['actual_models'],
                "channels_per_model": config['channels_per_model'],
                "total_frames": int(sum(m.num_frames for m in metrics)),
                "total_runtime": max(m.start_time for m in metrics) - min(m.start_time for m in metrics) if metrics else 0,
                "model_load_time": config.get('model_load_time', 0)
            },
            "performance_metrics": {},
            "hardware_analysis": {},
            "optimization_recommendations": {}
        }
        
        # æ€§èƒ½æŒ‡æ¨™ (é€™ä¸€æ®µä¸è®Š)
        if metrics:
            fps_values = [m.get_fps() for m in metrics if m.get_fps() > 0]
            latency_values = [m.get_latency_ms() for m in metrics if m.get_latency_ms() > 0]
            throughput_values = [m.get_throughput() for m in metrics if m.get_throughput() > 0]
            
            report["performance_metrics"] = {
                "fps": {
                    "average": float(np.mean(fps_values)) if fps_values else 0.0,
                    "min": float(np.min(fps_values)) if fps_values else 0.0,
                    "max": float(np.max(fps_values)) if fps_values else 0.0,
                    "total": float(np.sum(throughput_values)) if throughput_values else 0.0
                },
                "latency_ms": {
                    "average": float(np.mean(latency_values)) if latency_values else 0.0,
                    "min": float(np.min(latency_values)) if latency_values else 0.0,
                    "max": float(np.max(latency_values)) if latency_values else 0.0
                },
                "throughput": {
                    "total": float(np.sum(throughput_values)) if throughput_values else 0.0
                },
                "resource_usage": resource_stats
            }

        # --- ğŸ‘‡ ç°¡åŒ–ç‚ºåƒ…è™•ç†æ‰¹æ¬¡æ¨¡å¼çš„ã€Œå¾®è§€æ€§èƒ½å‰–æã€é‚è¼¯ --- ğŸ‘‡
        # å¾®è§€æ€§èƒ½å‰–æ (Profiling)
        profiling_details = {}
        if metrics and metrics[0].profiling_data and 'batch_size' in metrics[0].profiling_data:
            def _avg_ms(data, key=None):
                if not data:
                    return 0.0
                values = [d.get(key, 0) for d in data] if key else data
                return (sum(values) / len(values)) * 1000 if values else 0.0

            # --- æ‰¹æ¬¡æ¨¡å¼å ±å‘Š ---
            p_data = metrics[0].profiling_data
            batch_size = p_data.get('batch_size', 1)
            
            # ç²å–ç¸½æ‰¹æ¬¡æ™‚é–“
            batch_wall_ms = _avg_ms(p_data.get('batch_predict_wall_times', []))
            
            # è¨ˆç®— *æ”¤æ* æ™‚é–“
            amortized_wall_ms = batch_wall_ms / batch_size if batch_size > 0 else 0
            
            # å»ºç«‹å–®ä¸€çš„ profiling ç‰©ä»¶
            macro_data = {
                "batch_size": batch_size,
                "batch_read_total_avg_ms": _avg_ms(p_data.get('batch_read_total_times', [])),
                "batch_predict_wall_avg_ms": batch_wall_ms,
                "batch_predict_infer_avg_ms": _avg_ms(p_data.get('batch_predict_infer_times', [])),
                "batch_predict_post_avg_ms": _avg_ms(p_data.get('batch_predict_post_times', [])),
                "amortized_latency_per_frame_ms": amortized_wall_ms
            }
            
            # åˆä½µå®è§€å¯¦æ¸¬æ•¸æ“šå’Œå¾®è§€ç†è«–æ•¸æ“š
            profiling_details = {**macro_data, **micro_profiling}

        # --- ğŸ‘† æ›¿æ›çµæŸ --- ğŸ‘†
        
        # å°‡å‰–ææ•¸æ“šåŠ å…¥åˆ° performance_metrics ä¸­
        if profiling_details:
            report["performance_metrics"]["profiling_details"] = profiling_details
        
        # ... ( hardware_analysis å’Œ optimization_recommendations éƒ¨åˆ†éƒ½ä¸€æ¨£) ...
        # (ç¡¬é«”åˆ†æå’Œå„ªåŒ–å»ºè­°é‚è¼¯ä¸è®Š)
        report["hardware_analysis"] = {
            "hardware_specs": self.hardware_specs,
            "channel_allocation": {
                "requested_channels": config['requested_channels'],
                "actual_models": config['actual_models'],
                "channels_per_model": config['channels_per_model'],
                "allocation_efficiency": min(100.0, config['actual_models'] / config['requested_channels'] * 100),
                "is_ideal_config": config['actual_models'] >= config['requested_channels']
            },
            "memory_utilization": {
                "total_used_memory": config.get('load_info', {}).get('total_memory_usage', 0),
                "available_memory": self.hardware_specs.get('gpu_memory_gb', 0) if self.device != 'cpu' else self.hardware_specs.get('total_memory_gb', 0)
            }
        }
        
        recommendations = []
        recommendations.append(f"æ‰¹æ¬¡å¤§å°ç‚º {config['requested_channels']}ï¼Œä½¿ç”¨å–®ä¸€æ¨¡å‹é€²è¡Œè™•ç†ã€‚")
        report["optimization_recommendations"] = recommendations
        
        return report

    def _print_fixed_channel_report(self, report: Dict[str, Any]):
        """æ‰“å°å›ºå®šChannelå ±å‘Š"""
        print("\n" + "="*80)
        print("ğŸš€ Innodisk Computer Vision Benchmark æ¸¬è©¦å ±å‘Š v1.0")
        print("="*80)
        
        # æ¸¬è©¦é…ç½®
        config = report["configuration"]
        print(f"\nğŸ“Š æ¸¬è©¦é…ç½®:")
        print(f"  â€¢ æ¨¡å‹: {config['model']}")
        print(f"  â€¢ è¦–é »: {config['video']}")
        print(f"  â€¢ æ‰¹æ¬¡å¤§å° (Channels): {config['requested_channels']}")
        print(f"  â€¢ å¯¦éš›æ¨¡å‹æ•¸: {config['actual_models']}")
        print(f"  â€¢ æ¨¡å‹è¼‰å…¥æ™‚é–“: {config['model_load_time']:.3f}ç§’")
        print(f"  â€¢ ç¸½åŸ·è¡Œæ™‚é–“: {config['total_execution_time']:.3f}ç§’")
        print(f"  â€¢ æ¨¡å‹è¼¸å…¥å°ºå¯¸: {config['img_size']}x{config['img_size']}")
        print(f"  â€¢ è¦–é »è§£æåº¦: {config['video_resolution']}")
        print(f"  â€¢ è¦–é »FPS: {config['video_fps']:.2f}")
        print(f"  â€¢ ç½®ä¿¡åº¦é–¾å€¼: {config['conf']}")
        print(f"  â€¢ IoUé–¾å€¼: {config['iou']}")
        print(f"  â€¢ æ¸¬è©¦æŒçºŒæ™‚é–“: {config['seconds']}ç§’")
        print(f"  â€¢ è¨­å‚™: {config['device']}")
        
        # ç¡¬é«”è¦æ ¼
        hw_specs = config['hardware_specs']
        print(f"\nğŸ’» ç¡¬é«”è¦æ ¼:")
        print(f"  â€¢ CPUæ ¸å¿ƒæ•¸: {hw_specs['cpu_cores']}")
        print(f"  â€¢ CPUç·šç¨‹æ•¸: {hw_specs['cpu_threads']}")
        print(f"  â€¢ ç³»çµ±è¨˜æ†¶é«”: {hw_specs['total_memory_gb']:.1f} GB")
        if hw_specs['cuda_available']:
            print(f"  â€¢ GPUæ•¸é‡: {hw_specs['gpu_count']}")
            if hw_specs['gpu_count'] > 1:
                # å¤šGPUç’°å¢ƒï¼šåˆ†åˆ¥é¡¯ç¤ºæ¯å€‹GPU
                for gpu in hw_specs['gpus']:
                    print(f"  â€¢ GPU {gpu['id']}: {gpu['name']} ({gpu['memory_gb']:.1f} GB, Compute {gpu['compute_capability']})")
            else:
                # å–®GPUç’°å¢ƒï¼šä¿æŒåŸæœ‰æ ¼å¼
                print(f"  â€¢ GPUè¨˜æ†¶é«”: {hw_specs['gpu_memory_gb']:.1f} GB")
                print(f"  â€¢ GPUåç¨±: {hw_specs['gpu_name']}")
        
        # æ€§èƒ½æŒ‡æ¨™
        if "performance_metrics" in report and report["performance_metrics"]:
            perf = report["performance_metrics"]
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ¨™:")
            print(f"  â€¢ å¹³å‡æ¯Channel FPS: {perf['fps']['average']:.2f}")
            print(f"  â€¢ FPSç¯„åœ: {perf['fps']['min']:.2f} - {perf['fps']['max']:.2f}")
            print(f"  â€¢ ç¸½FPS (æ‰€æœ‰Channelåˆè¨ˆ): {perf['fps']['total']:.2f}")
            print(f"  â€¢ å¹³å‡å»¶é²: {perf['latency_ms']['average']:.2f}ms")
            print(f"  â€¢ å»¶é²ç¯„åœ: {perf['latency_ms']['min']:.2f} - {perf['latency_ms']['max']:.2f}ms")
            
            # è³‡æºä½¿ç”¨ç‡çµ±è¨ˆ
            if "resource_usage" in perf:
                res = perf["resource_usage"]
                print(f"\nğŸ’» è³‡æºä½¿ç”¨ç‡:")
                print(f"  â€¢ CPUä½¿ç”¨ç‡: å¹³å‡ {res['cpu']['average']:.1f}% (ç¯„åœ: {res['cpu']['min']:.1f}% - {res['cpu']['max']:.1f}%)")
                print(f"  â€¢ è¨˜æ†¶é«”ä½¿ç”¨ç‡: å¹³å‡ {res['memory']['average']:.1f}% (ç¯„åœ: {res['memory']['min']:.1f}% - {res['memory']['max']:.1f}%)")
                print(f"  â€¢ GPUä½¿ç”¨ç‡: å¹³å‡ {res['gpu']['average']:.1f}% (ç¯„åœ: {res['gpu']['min']:.1f}% - {res['gpu']['max']:.1f}%)")
        
        # ç¡¬é«”åˆ†æ
        hw_analysis = report["hardware_analysis"]
        print(f"\nğŸ” ç¡¬é«”åˆ†æ:")
        print(f"  â€¢ åŸ·è¡Œæ¨¡å¼: éœæ…‹æ‰¹æ¬¡è™•ç†")
        print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {hw_analysis['channel_allocation']['requested_channels']}")
        print(f"  â€¢ ä½¿ç”¨æ¨¡å‹æ•¸: {hw_analysis['channel_allocation']['actual_models']}")
        
        # print(f"  â€¢ ä¼°ç®—æ¨¡å‹è¨˜æ†¶é«”: {hw_analysis['memory_utilization']['estimated_model_memory']:.1f} GB") # å·²æ£„ç”¨
        print(f"  â€¢ ç¸½ä½¿ç”¨è¨˜æ†¶é«”: {hw_analysis['memory_utilization']['total_used_memory']:.1f} GB")
        print(f"  â€¢ å¯ç”¨è¨˜æ†¶é«”: {hw_analysis['memory_utilization']['available_memory']:.1f} GB")
        
        # å„ªåŒ–å»ºè­°
        if report["optimization_recommendations"]:
            print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            for i, recommendation in enumerate(report["optimization_recommendations"], 1):
                print(f"  {i}. {recommendation}")
        
        print("\n" + "="*80)

    def _save_report(self, report: Dict[str, Any], output_file: str):
        """ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨éœæ…‹æ‰¹æ¬¡è™•ç†çš„è¨ˆç®—æ©Ÿè¦–è¦ºåŸºæº–æ¸¬è©¦å·¥å…·ã€‚")
    parser.add_argument("--video", type=str, required=True, help="è¦–é »æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--model", type=str, default="yolov8n.pt", help="YOLO æ¨¡å‹åç¨±æˆ–è·¯å¾‘")
    parser.add_argument("-n", "--channels", type=int, default=4, help="ä¸¦è¡Œè™•ç†çš„Channelæ•¸ã€‚åœ¨å–®æ¬¡æ¸¬è©¦ä¸­ä½œç‚ºæ‰¹æ¬¡å¤§å°ï¼›åœ¨è‡ªå‹•å„ªåŒ–ä¸­ä½œç‚ºæœ€å¤§æ¸¬è©¦æ‰¹æ¬¡å¤§å°ã€‚")
    parser.add_argument("-m", "--models", type=int, help="[å·²æ£„ç”¨] æ­¤åƒæ•¸å°‡è¢«å¿½ç•¥ï¼Œæ¨¡å‹æ•¸å§‹çµ‚ç‚º1ã€‚")
    parser.add_argument("--auto-optimize", action="store_true", help="è‡ªå‹•æ¸¬è©¦å¾1åˆ°Nçš„æ‰¹æ¬¡å¤§å°ï¼Œä»¥æ‰¾åˆ°æœ€ä½³ååé‡ã€‚")
    
    parser.add_argument("-t", "--seconds", type=int, default=60, help="æ¸¬è©¦æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--img-size", type=int, default=640, help="æ¨¡å‹è¼¸å…¥å°ºå¯¸")
    parser.add_argument("--conf", type=float, default=0.25, help="ç½®ä¿¡åº¦é–¾å€¼")
    parser.add_argument("--iou", type=float, default=0.5, help="IoU é–¾å€¼")
    parser.add_argument("--device", type=str, default="cuda", help="è¨­å‚™é…ç½® (auto, cpu, cuda)")
    parser.add_argument("--output", type=str, help="è¼¸å‡ºå ±å‘Šæ–‡ä»¶è·¯å¾‘ (å–®æ¬¡æ¸¬è©¦) æˆ–å ±å‘Šç›®éŒ„ (è‡ªå‹•å„ªåŒ–)")
    parser.add_argument("--save-preview", action="store_true", help="Save the first batch of processed frames with bounding boxes for visual verification.")
    
    args = parser.parse_args()
    
    # å°‡ output åƒæ•¸ä½œç‚º output_dir å‚³éçµ¦è‡ªå‹•å„ªåŒ–
    if args.auto_optimize:
        args.output_dir = args.output if args.output else "reports"

    try:
        # å‰µå»ºåŸºæº–æ¸¬è©¦å™¨
        benchmark = FixedChannelBenchmark(
            model_name=args.model,
            device=args.device,
            img_size=args.img_size,
            conf_threshold=args.conf,
            iou_threshold=args.iou
        )
        
        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        if args.auto_optimize:
            # è‡ªå‹•å„ªåŒ–æ¨¡å¼ï¼šæ¸¬è©¦å¾æ‰¹æ¬¡å¤§å°1åˆ°N
            report = benchmark.run_auto_optimization(args)
        else:
            # å–®æ¬¡æ¸¬è©¦æ¨¡å¼
            report = benchmark.benchmark_video_fixed_channels(
                video_path=args.video,
                duration_seconds=args.seconds,
                requested_channels=args.channels,
                output_file=args.output,
                save_preview=args.save_preview
            )
        
        print("\nâœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
