#!/usr/bin/env python3
"""
å›ºå®šChannelæ•¸é‡çš„å¤šæ¨¡å‹ä¸¦è¡ŒåŸºæº–æ¸¬è©¦å·¥å…·
ç”¨æˆ¶è¨­å®šçš„Channelæ•¸ä¸æœƒæ”¹è®Šï¼Œç”¨å¯è¼‰å…¥çš„æ¨¡å‹æ•¸é‡ä¾†è™•ç†æ‰€æœ‰Channel
"""
import argparse
import os
import sys
import threading
import time
import psutil
import json
from time import perf_counter
from typing import List, Tuple, Optional, Dict, Any
from datetime import datetime
import numpy as np
import cv2
import torch
from collections import deque
import queue
from torch.profiler import profile, record_function, ProfilerActivity
try:
    import pynvml
except ImportError:
    pynvml = None

try:
    from ultralytics import YOLO
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥ ultralytics: {e}")
    print("è«‹å®‰è£ ultralytics: pip install ultralytics")
    sys.exit(1)


class FixedChannelMetric:
    """å›ºå®šChannelæ€§èƒ½æŒ‡æ¨™æ”¶é›†å™¨"""
    
    def __init__(self, channel_id: int = 0) -> None:
        self.channel_id = channel_id
        self.lock = threading.Lock()
        
        # åŸºæœ¬æŒ‡æ¨™
        self.num_frames: int = 0
        self.total_proc_s: float = 0.0
        self.start_time = time.time()
        
        # æ€§èƒ½æ­·å²
        self.processing_times: List[float] = []
        self.fps_history: List[float] = []
        
        # æª¢æ¸¬æŒ‡æ¨™
        self.detection_counts: List[int] = []
        
        # æ¨¡å‹åˆ†é…ä¿¡æ¯
        self.assigned_model_id: int = -1
        self.model_shared: bool = False
        self.profiling_data: Dict[str, Any] = {}

    def update(self, proc_s: float, detections: int = 0) -> None:
        """æ›´æ–°æ€§èƒ½æŒ‡æ¨™"""
        with self.lock:
            self.num_frames += 1
            self.total_proc_s += float(proc_s)
            self.processing_times.append(proc_s)
            
            # è¨ˆç®—ç•¶å‰FPS
            current_fps = 1.0 / proc_s if proc_s > 0 else 0.0
            self.fps_history.append(current_fps)
            
            # æª¢æ¸¬æŒ‡æ¨™
            self.detection_counts.append(detections)

    def get_fps(self) -> float:
        """è¨ˆç®—å¯¦éš›FPS"""
        with self.lock:
            if self.num_frames <= 0:
                return 0.0
            elapsed = time.time() - self.start_time
            if elapsed <= 0:
                return 0.0
            return self.num_frames / elapsed

    def get_latency_ms(self) -> float:
        """è¨ˆç®—å¹³å‡å»¶é²ï¼ˆæ¯«ç§’ï¼‰"""
        with self.lock:
            if self.num_frames <= 0:
                return 0.0
            return (self.total_proc_s / self.num_frames) * 1000.0

    def get_throughput(self) -> float:
        """è¨ˆç®—ç¸½ååé‡ï¼ˆæ¯ç§’è™•ç†çš„å¹€æ•¸ï¼‰"""
        with self.lock:
            elapsed = time.time() - self.start_time
            if elapsed <= 0:
                return 0.0
            return self.num_frames / elapsed

    def get_avg_detections(self) -> float:
        """è¨ˆç®—å¹³å‡æª¢æ¸¬æ•¸é‡"""
        with self.lock:
            if not self.detection_counts:
                return 0.0
            return sum(self.detection_counts) / len(self.detection_counts)

class ResourceMonitor(threading.Thread):
    """è³‡æºç›£æ§å™¨ï¼Œç”¨æ–¼åœ¨æ¸¬è©¦æœŸé–“æ”¶é›†ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³"""
    def __init__(self, sample_interval: float = 1.0):
        super().__init__()
        self.daemon = True
        self._stop_event = threading.Event()
        self.sample_interval = sample_interval
        
        self.cpu_usage: List[float] = []
        self.memory_usage: List[float] = []
        self.gpu_usage: List[float] = []
        
        self._pynvml_initialized = False
        if pynvml:
            try:
                pynvml.nvmlInit()
                self._pynvml_initialized = True
            except pynvml.NVMLError:
                print("âš ï¸ ç„¡æ³•åˆå§‹åŒ– pynvmlï¼ŒGPU ä½¿ç”¨ç‡å°‡ä¸æœƒè¢«ç›£æ§ã€‚")

    def run(self) -> None:
        """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­å®šæœŸæ”¶é›†è³‡æºæ•¸æ“š"""
        while not self._stop_event.is_set():
            # æ”¶é›† CPU å’Œè¨˜æ†¶é«”ä½¿ç”¨ç‡
            self.cpu_usage.append(psutil.cpu_percent())
            self.memory_usage.append(psutil.virtual_memory().percent)
            
            # æ”¶é›† GPU ä½¿ç”¨ç‡
            gpu_percent = 0.0
            if self._pynvml_initialized:
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    gpu_percent = util.gpu
                except pynvml.NVMLError:
                    gpu_percent = 0.0 # å¦‚æœå‡ºéŒ¯ï¼Œå‰‡è¨˜éŒ„ç‚º0
            self.gpu_usage.append(gpu_percent)
            
            time.sleep(self.sample_interval)

    def stop(self) -> None:
        """åœæ­¢è³‡æºç›£æ§"""
        self._stop_event.set()
        if self._pynvml_initialized:
            try:
                pynvml.nvmlShutdown()
            except pynvml.NVMLError:
                pass

    def get_stats(self) -> Dict[str, Dict[str, float]]:
        """è¨ˆç®—ä¸¦è¿”å›è³‡æºä½¿ç”¨çš„çµ±è¨ˆæ•¸æ“š"""
        def _calculate(data: List[float]) -> Dict[str, float]:
            if not data:
                return {"average": 0.0, "min": 0.0, "max": 0.0}
            return {
                "average": float(np.mean(data)) if data else 0.0,
                "min": float(np.min(data)) if data else 0.0,
                "max": float(np.max(data)) if data else 0.0
            }

        return {
            "cpu": _calculate(self.cpu_usage),
            "memory": _calculate(self.memory_usage),
            "gpu": _calculate(self.gpu_usage)
        }


class FixedChannelBenchmark:
    """å›ºå®šChannelæ•¸é‡çš„å¤šæ¨¡å‹ä¸¦è¡ŒåŸºæº–æ¸¬è©¦ä¸»é¡"""
    
    def __init__(self, 
                 model_name: str = 'yolov8n.pt',
                 device: str = 'auto',
                 img_size: int = 640,
                 conf_threshold: float = 0.25,
                 iou_threshold: float = 0.5):
        """åˆå§‹åŒ–å›ºå®šChannelåŸºæº–æ¸¬è©¦å™¨"""
        self.model_name = model_name
        self.device = self._parse_device(device)
        self.img_size = img_size
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # ç¡¬é«”è¦æ ¼æª¢æ¸¬
        self.hardware_specs = self._detect_hardware_specs()
        
        print(f"ğŸš€ å›ºå®šChannelå¤šæ¨¡å‹ä¸¦è¡ŒåŸºæº–æ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   â€¢ æ¨¡å‹: {model_name}")
        print(f"   â€¢ è¨­å‚™: {self.device}")
        print(f"   â€¢ åœ–ç‰‡å°ºå¯¸: {img_size}x{img_size}")
        print(f"   â€¢ ç½®ä¿¡åº¦é–¾å€¼: {conf_threshold}")
        print(f"   â€¢ IoU é–¾å€¼: {iou_threshold}")
        print(f"   â€¢ ç¡¬é«”è¦æ ¼: {self.hardware_specs}")

    def _parse_device(self, device: str) -> str:
        """è§£æè¨­å‚™é…ç½®"""
        if device == 'auto':
            if torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        return device

    def _detect_hardware_specs(self) -> Dict[str, Any]:
        """æª¢æ¸¬ç¡¬é«”è¦æ ¼"""
        specs = {
            'cpu_cores': psutil.cpu_count(logical=False),
            'cpu_threads': psutil.cpu_count(logical=True),
            'total_memory_gb': psutil.virtual_memory().total / (1024**3),
            'cuda_available': torch.cuda.is_available(),
            'gpu_count': 0,
            'gpu_memory_gb': 0,
            'gpu_name': 'Unknown',
            'gpus': []
        }
        
        if torch.cuda.is_available():
            specs['gpu_count'] = torch.cuda.device_count()
            if specs['gpu_count'] > 0:
                # æª¢æ¸¬æ‰€æœ‰GPU
                for i in range(specs['gpu_count']):
                    gpu_props = torch.cuda.get_device_properties(i)
                    gpu_info = {
                        'id': i,
                        'name': gpu_props.name,
                        'memory_gb': gpu_props.total_memory / (1024**3),
                        'compute_capability': f"{gpu_props.major}.{gpu_props.minor}"
                    }
                    specs['gpus'].append(gpu_info)
                
                # ä¿æŒå‘å¾Œå…¼å®¹æ€§
                specs['gpu_memory_gb'] = specs['gpus'][0]['memory_gb']
                specs['gpu_name'] = specs['gpus'][0]['name']
        
        return specs


    def _test_model_loading(self, num_models: int) -> Tuple[bool, List[float], List[float]]:
        """æ¸¬è©¦è¼‰å…¥æŒ‡å®šæ•¸é‡çš„æ¨¡å‹"""
        print(f"ğŸ§ª æ¸¬è©¦è¼‰å…¥ {num_models} å€‹æ¨¡å‹...")
        
        models = []
        load_times = []
        memory_usage = []
        
        try:
            for i in range(num_models):
                print(f"   ğŸ”„ è¼‰å…¥æ¨¡å‹ {i+1}/{num_models}...")
                
                start_time = perf_counter()
                model = YOLO(self.model_name)
                
                if self.device != 'cpu':
                    model.to(self.device)
                
                load_time = perf_counter() - start_time
                load_times.append(load_time)
                
                # ç²å–è¨˜æ†¶é«”ä½¿ç”¨é‡
                if torch.cuda.is_available():
                    mem_usage = torch.cuda.memory_allocated() / (1024**3)
                    memory_usage.append(mem_usage)
                else:
                    memory_usage.append(0.0)
                
                models.append(model)
                print(f"   âœ… æ¨¡å‹ {i+1} è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.3f}ç§’")
            
            # æ¸…ç†æ¨¡å‹
            for model in models:
                del model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return True, load_times, memory_usage
            
        except Exception as e:
            print(f"   âŒ è¼‰å…¥å¤±æ•—: {e}")
            # æ¸…ç†å·²è¼‰å…¥çš„æ¨¡å‹
            for model in models:
                try:
                    del model
                except:
                    pass
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return False, load_times, memory_usage

    def _find_max_loadable_models(self, requested_channels: int) -> Tuple[int, Dict[str, Any]]:
        """å°‹æ‰¾æœ€å¤§å¯è¼‰å…¥çš„æ¨¡å‹æ•¸é‡ï¼ˆä¸Šé™ç‚ºChannelæ•¸é‡ï¼‰"""
        print(f"\nğŸ” å°‹æ‰¾æœ€å¤§å¯è¼‰å…¥æ¨¡å‹æ•¸é‡...")
        print(f"   â€¢ è«‹æ±‚Channelæ•¸: {requested_channels}")
        
        # å¾è«‹æ±‚çš„Channelæ•¸é–‹å§‹å¾€ä¸‹æ¸¬è©¦
        max_models = requested_channels
        print(f"   â€¢ èµ·å§‹æ¸¬è©¦æ¨¡å‹æ•¸: {max_models}")
        
        if max_models <= 0:
            print("   âŒ ç¡¬é«”è¦æ ¼ä¸è¶³ä»¥è¼‰å…¥ä»»ä½•æ¨¡å‹")
            return 0, {}
        
        # å¾ç†è«–æœ€å¤§å€¼é–‹å§‹ï¼Œé€æ­¥éæ¸›æ¸¬è©¦
        # å¾ç†è«–æœ€å¤§å€¼é–‹å§‹ï¼Œé€æ­¥éæ¸›æ¸¬è©¦
        for count in range(max_models, 0, -1):
            print(f"\n   ğŸ§ª æ¸¬è©¦ {count} å€‹æ¨¡å‹...")
            
            success, load_times, memory_usage = self._test_model_loading(count)
            
            if success:
                print(f"   âœ… æˆåŠŸè¼‰å…¥ {count} å€‹æ¨¡å‹")
                print(f"   â€¢ å¹³å‡è¼‰å…¥æ™‚é–“: {np.mean(load_times):.3f}ç§’")
                print(f"   â€¢ å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨: {np.mean(memory_usage):.2f} GB")
                
                return count, {
                    'load_times': load_times,
                    'memory_usage': memory_usage,
                    'avg_load_time': float(np.mean(load_times)) if load_times else 0.0,
                    'avg_memory_usage': float(np.mean(memory_usage)) if memory_usage else 0.0,
                    'total_memory_usage': float(np.sum(memory_usage)) if memory_usage else 0.0
                }
            else:
                print(f"   âŒ ç„¡æ³•è¼‰å…¥ {count} å€‹æ¨¡å‹")
        
        print("   âŒ ç„¡æ³•è¼‰å…¥ä»»ä½•æ¨¡å‹")
        return 0, {}

    def _create_model_instance(self, model_id: int) -> Tuple[YOLO, float, float]:
        """å‰µå»ºæ¨¡å‹å¯¦ä¾‹"""
        print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹å¯¦ä¾‹ {model_id}...")
        
        start_time = perf_counter()
        
        try:
            # å‰µå»ºæ–°çš„æ¨¡å‹å¯¦ä¾‹
            yolo_model = YOLO(self.model_name)
            
            if self.device != 'cpu':
                yolo_model.to(self.device)
            
            load_time = perf_counter() - start_time
            
            # ç²å–æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨é‡
            memory_usage = 0.0
            if torch.cuda.is_available():
                memory_usage = torch.cuda.memory_allocated() / (1024**3)  # GB
            
            print(f"âœ… æ¨¡å‹å¯¦ä¾‹ {model_id} è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.3f}ç§’")
            
            return yolo_model, load_time, memory_usage
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹å¯¦ä¾‹ {model_id} è¼‰å…¥å¤±æ•—: {e}")
            raise

    def predict_single_frame(self, model: YOLO, frame: np.ndarray) -> Tuple[List[Dict], float]:
        """
        [å®è§€æ¸¬è©¦ç”¨] å°å–®ä¸€å¹€é€²è¡Œé æ¸¬ï¼Œåªè¿”å›æª¢æ¸¬çµæœå’Œç¸½ç‰†ä¸Šæ™‚é–“ï¼ˆç§’ï¼‰ã€‚
        Profiler å·²è¢«ç§»é™¤ï¼Œä»¥ç¢ºä¿åŸ·è¡Œç·’å®‰å…¨ã€‚
        
        è¿”å›:
            detections (List[Dict]): æª¢æ¸¬çµæœ
            processing_time_s (float): ç¸½ç‰†ä¸Šæ™‚é–“ (ç§’)
        """
        t_wall_start = perf_counter()

        try:
            # 1. åŸ·è¡Œæ¨è«– (ä¸ä½¿ç”¨ profiler)
            with torch.inference_mode():
                results = model.predict(
                    source=frame,
                    conf=self.conf_threshold,
                    iou=self.iou_threshold,
                    imgsz=self.img_size,
                    verbose=False,
                    save=False
                )

            # 2. CPU å¾Œè™•ç†
            detections = []
            if results:
                for r in results:
                    if r.boxes is not None:
                        boxes = r.boxes.xyxy.cpu().numpy()
                        confidences = r.boxes.conf.cpu().numpy()
                        classes = r.boxes.cls.cpu().numpy().astype(int)
                        
                        for i in range(len(boxes)):
                            detection = {
                                'class_id': int(classes[i]),
                                'confidence': float(confidences[i]),
                                'bbox': boxes[i].tolist(),
                                'class_name': r.names[int(classes[i])]
                            }
                            detections.append(detection)

            t_wall_end = perf_counter()
            processing_time_s = t_wall_end - t_wall_start
            
            return detections, processing_time_s
            
        except Exception as e:
            print(f"âš ï¸ é æ¸¬éŒ¯èª¤: {e}")
            return [], 0.0

    def _profile_model_once(self, model: YOLO) -> Dict[str, float]:
        """
        [å¾®è§€å‰–æç”¨] åœ¨ä¸»åŸ·è¡Œç·’ä¸­å°å–®ä¸€æ¨¡å‹å¯¦ä¾‹é€²è¡Œè©³ç´°å‰–æã€‚
        é€™æœƒé ç†±ä¸¦é‹è¡Œå¤šæ¬¡æ¨è«–ï¼Œä»¥ç²å–ç©©å®šçš„ GPU é‹ç®—/I/O ç†è«–å€¼ã€‚
        
        è¿”å›:
            Dict[str, float]: åŒ…å« 'gpu_compute_avg_ms', 'gpu_io_avg_ms', 'cpu_post_proc_avg_ms' çš„å­—å…¸
        """
        print(f"   ğŸ”¬ [å¾®è§€å‰–æ] é–‹å§‹å° {self.model_name} é€²è¡Œå–®æ¨¡å‹ç†è«–å€¼åˆ†æ...")
        
        use_cuda = torch.cuda.is_available() and self.device != 'cpu'
        if not use_cuda:
            print("   âš ï¸ [å¾®è§€å‰–æ] æœªä½¿ç”¨ CUDAï¼Œè·³éè©³ç´°å‰–æã€‚")
            return {}

        # å‰µå»ºä¸€å€‹ç¬¦åˆ img_size çš„å‡ (dummy) åœ–åƒ
        dummy_frame = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)
        
        warmup_runs = 20
        profile_runs = 50
        
        results_compute: List[float] = []
        results_io: List[float] = []
        results_post_proc: List[float] = []

        try:
            # 1. é ç†± (Warm-up)
            print(f"   ğŸ”¬ [å¾®è§€å‰–æ] åŸ·è¡Œ {warmup_runs} æ¬¡é ç†±...")
            with torch.inference_mode():
                for _ in range(warmup_runs):
                    _ = model.predict(source=dummy_frame, verbose=False)
            
            # 2. å‰–æ (Profiling)
            print(f"   ğŸ”¬ [å¾®è§€å‰–æ] åŸ·è¡Œ {profile_runs} æ¬¡å‰–æ...")
            for _ in range(profile_runs):
                gpu_compute_s = 0.0
                gpu_io_s = 0.0
                
                with torch.inference_mode():
                    with profile(
                        activities=[ProfilerActivity.CUDA], # æˆ‘å€‘åªé—œå¿ƒ CUDA äº‹ä»¶
                        record_shapes=False,
                        with_stack=False
                    ) as prof:
                        results = model.predict(
                            source=dummy_frame,
                            conf=self.conf_threshold,
                            iou=self.iou_threshold,
                            imgsz=self.img_size,
                            verbose=False,
                            save=False
                        )
                
                # æå– Profiler æ•¸æ“š
                for event in prof.events():
                    if "memcpy" in event.name.lower():
                        gpu_io_s += event.cuda_time_total / 1_000_000.0  # us -> s
                    elif "kernel" in event.name.lower():
                        gpu_compute_s += event.cuda_time_total / 1_000_000.0 # us -> s
                
                # æ¸¬é‡ CPU å¾Œè™•ç†
                t_post_start = perf_counter()
                if results:
                    for r in results:
                        _ = r.boxes.xyxy.cpu().numpy() # æ¨¡æ“¬å¾Œè™•ç†
                cpu_post_proc_s = perf_counter() - t_post_start
                
                results_compute.append(gpu_compute_s)
                results_io.append(gpu_io_s)
                results_post_proc.append(cpu_post_proc_s)
            
            # 3. è¨ˆç®—å¹³å‡å€¼ä¸¦è½‰æ›ç‚ºæ¯«ç§’ (ms)
            avg_compute_ms = (sum(results_compute) / len(results_compute)) * 1000
            avg_io_ms = (sum(results_io) / len(results_io)) * 1000
            avg_post_proc_ms = (sum(results_post_proc) / len(results_post_proc)) * 1000
            
            result_dict = {
                "micro_gpu_compute_avg_ms": avg_compute_ms,
                "micro_gpu_io_avg_ms": avg_io_ms,
                "micro_cpu_post_proc_avg_ms": avg_post_proc_ms,
                "micro_total_avg_ms": avg_compute_ms + avg_io_ms + avg_post_proc_ms
            }
            print(f"   âœ… [å¾®è§€å‰–æ] å®Œæˆ: {result_dict}")
            return result_dict
            
        except Exception as e:
            print(f"   âŒ [å¾®è§€å‰–æ] å¤±æ•—: {e}")
            return {}

    def benchmark_video_fixed_channels(self, 
                                      video_path: str, 
                                      duration_seconds: int = 60,
                                      requested_channels: int = 1,
                                      fixed_models: Optional[int] = None,
                                      output_file: Optional[str] = None) -> Dict[str, Any]:
        """å›ºå®šChannelæ•¸é‡çš„å¤šæ¨¡å‹ä¸¦è¡Œè¦–é »åŸºæº–æ¸¬è©¦"""
        # è¨˜éŒ„æ¸¬è©¦é–‹å§‹æ™‚é–“
        test_start_time = time.time()
        
        print(f"ğŸ¬ é–‹å§‹å›ºå®šChannelå¤šæ¨¡å‹ä¸¦è¡Œè¦–é »åŸºæº–æ¸¬è©¦")
        print(f"   â€¢ è¦–é »: {video_path}")
        print(f"   â€¢ æŒçºŒæ™‚é–“: {duration_seconds}ç§’")
        print(f"   â€¢ è«‹æ±‚Channelæ•¸: {requested_channels}")
        
        # é©—è­‰è¦–é »æ–‡ä»¶
        if not os.path.isfile(video_path):
            raise FileNotFoundError(f"è¦–é »æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # ç²å–è¦–é »ä¿¡æ¯
        video_info = self._get_video_info(video_path)
        print(f"ğŸ“¹ è¦–é »ä¿¡æ¯: {video_info['width']}x{video_info['height']}, {video_info['fps']:.2f} FPS")
        
        # ç¢ºå®šè¦è¼‰å…¥çš„æ¨¡å‹æ•¸é‡
        if fixed_models is not None:
            # ä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„å›ºå®šæ¨¡å‹æ•¸é‡
            max_models = fixed_models
            print(f"\nğŸ”§ ä½¿ç”¨å›ºå®šæ¨¡å‹æ•¸é‡: {max_models}")
            print(f"   â€¢ è·³éè‡ªå‹•è¨ˆç®—ï¼Œç›´æ¥è¼‰å…¥ {max_models} å€‹æ¨¡å‹")
            
            # æ¸¬è©¦è¼‰å…¥æŒ‡å®šæ•¸é‡çš„æ¨¡å‹
            success, load_times, memory_usage = self._test_model_loading(max_models)
            if not success:
                print(f"âŒ ç„¡æ³•è¼‰å…¥ {max_models} å€‹æ¨¡å‹ï¼Œæ¸¬è©¦çµ‚æ­¢")
                return {}
            
            load_info = {
                'load_times': load_times,
                'memory_usage': memory_usage,
                'avg_load_time': float(np.mean(load_times)) if load_times else 0.0,
                'avg_memory_usage': float(np.mean(memory_usage)) if memory_usage else 0.0,
                'total_memory_usage': float(np.sum(memory_usage)) if memory_usage else 0.0
            }
        else:
            # ä½¿ç”¨è‡ªå‹•è¨ˆç®—çš„æ¨¡å‹æ•¸é‡
            max_models, load_info = self._find_max_loadable_models(requested_channels)
        
        if max_models == 0:
            print("âŒ ç„¡æ³•è¼‰å…¥ä»»ä½•æ¨¡å‹ï¼Œæ¸¬è©¦çµ‚æ­¢")
            return {}
        
        print(f"\nğŸ¯ æ¨¡å‹åˆ†é…ç­–ç•¥:")
        print(f"   â€¢ è«‹æ±‚Channelæ•¸: {requested_channels}")
        print(f"   â€¢ è¼‰å…¥æ¨¡å‹æ•¸: {max_models}")
        
        if fixed_models is not None:
            print(f"   â€¢ é…ç½®æ–¹å¼: ç”¨æˆ¶æŒ‡å®šå›ºå®šæ¨¡å‹æ•¸é‡")
        else:
            print(f"   â€¢ é…ç½®æ–¹å¼: è‡ªå‹•è¨ˆç®—æ¨¡å‹æ•¸é‡")
        
        if max_models >= requested_channels:
            print(f"   â€¢ åˆ†é…ç­–ç•¥: æ¯å€‹Channeléƒ½æœ‰å°ˆå±¬æ¨¡å‹ (ç†æƒ³é…ç½®)")
            channels_per_model = 1.0
        else:
            print(f"   â€¢ åˆ†é…ç­–ç•¥: {max_models}å€‹æ¨¡å‹è™•ç†{requested_channels}å€‹Channel")
            channels_per_model = requested_channels / max_models
            print(f"   â€¢ æ¯å€‹æ¨¡å‹è™•ç†: {channels_per_model:.1f}å€‹Channel")
            
            # è§£é‡‹ç‚ºä»€éº¼é™åˆ¶æ¨¡å‹æ•¸é‡
            if fixed_models is not None:
                print(f"   â€¢ åŸå› : ç”¨æˆ¶æŒ‡å®šå›ºå®šæ¨¡å‹æ•¸é‡")
            elif requested_channels > 8:
                print(f"   â€¢ åŸå› : é¿å…GPUé‹ç®—ç“¶é ¸ï¼Œç¢ºä¿æœ€ä½³æ€§èƒ½")
        
        # åˆå§‹åŒ–ChannelæŒ‡æ¨™æ”¶é›†å™¨
        channel_metrics = [FixedChannelMetric(i) for i in range(requested_channels)]
        
        # è¼‰å…¥æ¨¡å‹å¯¦ä¾‹
        print(f"\nğŸ”„ è¼‰å…¥ {max_models} å€‹æ¨¡å‹å¯¦ä¾‹...")
        models = []
        model_load_times = []
        model_memory_usage = []
        
        for i in range(max_models):
            model, load_time, memory_usage = self._create_model_instance(i)
            models.append(model)
            model_load_times.append(load_time)
            model_memory_usage.append(memory_usage)
        
        print(f"âœ… æ‰€æœ‰æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œç¸½è€—æ™‚: {sum(model_load_times):.3f}ç§’")
        
        # --- åŸ·è¡Œä¸€æ¬¡å¾®è§€å‰–æ (ä»»å‹™ A) ---
        micro_profiling_results = {}
        if models:
            micro_profiling_results = self._profile_model_once(models[0])
        else:
            print("   âš ï¸ æ²’æœ‰è¼‰å…¥ä»»ä½•æ¨¡å‹ï¼Œè·³éå¾®è§€å‰–æã€‚")
            
        # å‰µå»ºChannelåˆ†é…æ˜ å°„
        channel_to_model = {}
        for channel_id in range(requested_channels):
            if max_models >= requested_channels:
                # ç†æƒ³é…ç½®ï¼šæ¯å€‹Channeléƒ½æœ‰å°ˆå±¬æ¨¡å‹
                model_id = channel_id
                channel_metrics[channel_id].model_shared = False
            else:
                # æ¨¡å‹å…±äº«ï¼šå¤šå€‹Channelå…±äº«æ¨¡å‹
                model_id = channel_id % max_models
                channel_metrics[channel_id].model_shared = True
            
            channel_to_model[channel_id] = model_id
            channel_metrics[channel_id].assigned_model_id = model_id
        
        print(f"ğŸ“‹ Channelåˆ†é…æ˜ å°„:")
        for channel_id in range(requested_channels):
            model_id = channel_to_model[channel_id]
            print(f"   â€¢ Channel {channel_id} â†’ Model {model_id}")
        
        # åˆå§‹åŒ–ä¸¦å•Ÿå‹•è³‡æºç›£æ§å™¨
        resource_monitor = ResourceMonitor()
        resource_monitor.start()

        # å•Ÿå‹•Channelå·¥ä½œç·šç¨‹
        threads = []
        stop_ts = time.time() + duration_seconds
        
        print(f"\nğŸš€ å•Ÿå‹• {requested_channels} å€‹Channelå·¥ä½œç·šç¨‹...")
        
        for channel_id in range(requested_channels):
            model_id = channel_to_model[channel_id]
            thread = threading.Thread(
                target=self._fixed_channel_worker_thread,
                args=(channel_id, model_id, video_path, stop_ts, channel_metrics[channel_id], models[model_id]),
                daemon=True
            )
            thread.start()
            threads.append(thread)
        
        print("âœ… é–‹å§‹å›ºå®šChannelæ€§èƒ½ç›£æ§\n")
        
        # å®šæœŸå ±å‘Š
        self._fixed_channel_monitor_progress(channel_metrics, stop_ts)

        # ç­‰å¾…æ‰€æœ‰ç·šç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # åœæ­¢è³‡æºç›£æ§ä¸¦ç²å–æ•¸æ“š
        resource_monitor.stop()
        resource_monitor.join()
        resource_stats = resource_monitor.get_stats()
        
        # æ¸…ç†æ¨¡å‹
        for model in models:
            del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
        test_end_time = time.time()
        total_execution_time = test_end_time - test_start_time
        
        # --- ğŸ‘‡ é€™è£¡æ˜¯ä¿®æ­£é» #1 (å¤±èª¤ #1) --- ğŸ‘‡
        # ç”Ÿæˆå ±å‘Š (è£œä¸Šå®Œæ•´çš„ config å­—å…¸)
        config = {
            'model': self.model_name,
            'video': video_path,
            'requested_channels': requested_channels,
            'actual_models': max_models,
            'channels_per_model': channels_per_model,
            'fixed_models': fixed_models,
            'img_size': self.img_size,
            'video_resolution': f"{video_info['width']}x{video_info['height']}",
            'video_fps': video_info['fps'],
            'conf': self.conf_threshold,
            'iou': self.iou_threshold,
            'seconds': duration_seconds,
            'device': self.device,
            'model_load_time': sum(model_load_times),
            'total_execution_time': total_execution_time,
            'architecture': 'fixed_channel_multi_model_parallel',
            'hardware_specs': self.hardware_specs,
            'load_info': load_info,
            'channel_allocation': channel_to_model
        }
        
        # å°‡å¾®è§€å‰–æçµæœ (micro_profiling_results) å‚³éçµ¦å ±å‘Šç”Ÿæˆå™¨
        report = self._generate_fixed_channel_report(
            channel_metrics, config, resource_stats, micro_profiling_results
        )
        # --- ğŸ‘† ä¿®æ­£çµæŸ --- ğŸ‘†
        
        self._print_fixed_channel_report(report)
        
        # è‡ªå‹•ç”Ÿæˆå ±å‘Šæª”æ¡ˆåï¼ˆå¦‚æœæ²’æœ‰æŒ‡å®šï¼‰
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = os.path.splitext(os.path.basename(self.model_name))[0]
            output_file = f"reports/cv_benchmark_{model_name}_{requested_channels}ch_{timestamp}.json"
        
        # ç¢ºä¿ reports ç›®éŒ„å­˜åœ¨
        os.makedirs("reports", exist_ok=True)
        
        # ä¿å­˜å ±å‘Š
        self._save_report(report, output_file)
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {output_file}")
        
        return report

    def run_auto_optimization(self, args: argparse.Namespace) -> Dict[str, Any]:
        """
        è‡ªå‹•å„ªåŒ–ä¸»å‡½æ•¸ï¼Œè¿­ä»£ä¸åŒçš„æ¨¡å‹æ•¸é‡é…ç½®ï¼ŒåŸ·è¡Œæ¸¬è©¦ï¼Œä¸¦ç”Ÿæˆæœ€ä½³åŒ–å ±å‘Šã€‚
        """
        # è¨˜éŒ„å„ªåŒ–æ¸¬è©¦é–‹å§‹æ™‚é–“
        optimization_start_time = time.time()
        
        print(f"ğŸš€ é–‹å§‹è‡ªå‹•å„ªåŒ–æ¨¡å‹æ•¸é‡æ¸¬è©¦")
        print(f"   â€¢ è¦–é »: {args.video}")
        print(f"   â€¢ æŒçºŒæ™‚é–“: {args.seconds}ç§’")
        print(f"   â€¢ è«‹æ±‚Channelæ•¸: {args.channels}")
        
        # ç²å–è¦–é »ä¿¡æ¯
        video_info = self._get_video_info(args.video)
        print(f"ğŸ“¹ è¦–é »ä¿¡æ¯: {video_info['width']}x{video_info['height']}, {video_info['fps']:.2f} FPS")
        
        # å‰µå»ºå”¯ä¸€çš„å ±å‘Šç›®éŒ„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name_base = os.path.splitext(os.path.basename(self.model_name))[0]
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å®šçš„ output_dir
        output_dir = getattr(args, 'output_dir', 'reports')
        
        report_dir_name = f"cv_optimization_{model_name_base}_{args.channels}ch_{timestamp}"
        report_dir = os.path.join(output_dir, report_dir_name)
        os.makedirs(report_dir, exist_ok=True)
        print(f"ğŸ“‚ å ±å‘Šå°‡å„²å­˜æ–¼: {report_dir}")
        
        # è¿­ä»£æ¸¬è©¦é‚è¼¯
        test_results = []
        test_configs = list(range(1, args.channels + 1))
        
        print(f"\nğŸ” å°‡åŸ·è¡Œ {len(test_configs)} æ¬¡æ¸¬è©¦ï¼Œæ¨¡å‹æ•¸é‡å¾ 1 åˆ° {args.channels}")
        
        for i, model_count in enumerate(test_configs, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ§ª æ¸¬è©¦ {i}/{len(test_configs)}: ä½¿ç”¨ {model_count} å€‹æ¨¡å‹")
            print(f"{'='*60}")
            
            try:
                # ç‚ºä¸­é–“å ±å‘Šç”Ÿæˆæª”æ¡ˆè·¯å¾‘
                intermediate_report_name = f"benchmark_{model_count}_models.json"
                intermediate_output_file = os.path.join(report_dir, intermediate_report_name)
                
                # åŸ·è¡Œå–®æ¬¡æ¸¬è©¦
                result = self.benchmark_video_fixed_channels(
                    video_path=args.video,
                    duration_seconds=args.seconds,
                    requested_channels=args.channels,
                    fixed_models=model_count,
                    output_file=intermediate_output_file
                )
                
                if result and 'performance_metrics' in result:
                    perf = result['performance_metrics']
                    config = result['configuration']
                    resource_usage = perf.get('resource_usage', {})
                    
                    # æå–é—œéµæŒ‡æ¨™
                    avg_fps = perf['fps']['average']
                    total_fps = perf['fps']['total']
                    avg_latency = perf['latency_ms']['average']
                    channels_per_model = config['channels_per_model']
                    
                    # è¨ˆç®—æ•ˆç‡åˆ†æ•¸
                    efficiency_score = self._calculate_efficiency_score(
                        avg_fps, total_fps, avg_latency,
                        args.channels, model_count, channels_per_model
                    )
                    
                    summary = {
                        'model_count': model_count,
                        'avg_fps': avg_fps,
                        'total_fps': total_fps,
                        'avg_latency': avg_latency,
                        'channels_per_model': channels_per_model,
                        'efficiency_score': efficiency_score,
                        'is_ideal_config': model_count >= args.channels,
                        'resource_usage': resource_usage,
                        'report_file': intermediate_report_name
                    }
                    
                    # å¦‚æœå­˜åœ¨ profiling_detailsï¼Œå‰‡å°‡å…¶è¤‡è£½åˆ°æ‘˜è¦ä¸­
                    if 'profiling_details' in perf:
                        summary['profiling_details'] = perf['profiling_details']
                    
                    test_results.append(summary)
                    
                    print(f"âœ… æ¸¬è©¦å®Œæˆ: {model_count}å€‹æ¨¡å‹")
                    print(f"   â€¢ å¹³å‡FPS: {avg_fps:.2f}")
                    print(f"   â€¢ ç¸½FPS: {total_fps:.2f}")
                    print(f"   â€¢ å¹³å‡å»¶é²: {avg_latency:.2f}ms")
                    print(f"   â€¢ æ•ˆç‡åˆ†æ•¸: {efficiency_score:.2f}")
                    
                else:
                    print(f"âŒ æ¸¬è©¦å¤±æ•—: {model_count}å€‹æ¨¡å‹")
                    
            except Exception as e:
                print(f"âŒ æ¸¬è©¦éŒ¯èª¤: {model_count}å€‹æ¨¡å‹ - {e}")
                continue
        
        # åˆ†æçµæœä¸¦æ‰¾åˆ°æœ€ä½³é…ç½®
        if not test_results:
            print("âŒ æ‰€æœ‰æ¸¬è©¦éƒ½å¤±æ•—äº†ï¼Œç„¡æ³•ç”Ÿæˆå„ªåŒ–å ±å‘Š")
            return {}
        
        best_config = self._find_best_configuration(test_results, args.channels)
        
        # ç”Ÿæˆå„ªåŒ–å ±å‘Š
        optimization_report = self._generate_optimization_report(
            test_results, best_config, video_info, args.channels
        )
        
        # é¡¯ç¤ºå„ªåŒ–çµæœ
        self._print_optimization_report(optimization_report)
        
        # å°‡æœ€çµ‚å„ªåŒ–å ±å‘Šå„²å­˜åˆ°å°ˆå±¬è³‡æ–™å¤¾ä¸­
        final_report_name = "optimization_report.json"
        final_output_file = os.path.join(report_dir, final_report_name)
        
        # ä¿å­˜å ±å‘Š
        self._save_report(optimization_report, final_output_file)
        print(f"\nğŸ“„ å„ªåŒ–å ±å‘Šå·²ä¿å­˜è‡³: {final_output_file}")
        
        return optimization_report

    def _calculate_efficiency_score(self, avg_fps: float, total_fps: float, 
                                  avg_latency: float, requested_channels: int, 
                                  model_count: int, channels_per_model: float) -> float:
        """è¨ˆç®—æ•ˆç‡åˆ†æ•¸"""
        # æ¬Šé‡é…ç½®
        fps_weight = 0.4      # FPSæ¬Šé‡
        latency_weight = 0.3  # å»¶é²æ¬Šé‡
        efficiency_weight = 0.3  # æ•ˆç‡æ¬Šé‡
        
        # FPSåˆ†æ•¸ (0-100)
        fps_score = min(100, (avg_fps / 30) * 100)  # ä»¥30 FPSç‚ºæ»¿åˆ†
        
        # å»¶é²åˆ†æ•¸ (0-100ï¼Œå»¶é²è¶Šä½åˆ†æ•¸è¶Šé«˜)
        latency_score = max(0, 100 - (avg_latency / 100) * 100)  # ä»¥100msç‚ºåŸºæº–
        
        # æ•ˆç‡åˆ†æ•¸ (0-100ï¼Œæ¨¡å‹åˆ©ç”¨ç‡è¶Šé«˜åˆ†æ•¸è¶Šé«˜)
        if channels_per_model >= 1.0:
            efficiency_score = 100  # ç†æƒ³é…ç½®
        else:
            efficiency_score = channels_per_model * 100  # å…±äº«æ¨¡å‹æ•ˆç‡
        
        # è¨ˆç®—ç¸½åˆ†
        total_score = (fps_score * fps_weight + 
                      latency_score * latency_weight + 
                      efficiency_score * efficiency_weight)
        
        return total_score

    def _find_best_configuration(self, results: List[Dict], requested_channels: int) -> Dict:
        """æ‰¾åˆ°æœ€ä½³é…ç½®"""
        if not results:
            return {}
        
        # æŒ‰æ•ˆç‡åˆ†æ•¸æ’åº
        sorted_results = sorted(results, key=lambda x: x['efficiency_score'], reverse=True)
        
        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best = sorted_results[0]
        
        # åˆ†æé…ç½®é¡å‹
        if best['is_ideal_config']:
            config_type = "ç†æƒ³é…ç½®"
            recommendation = "æ¯å€‹Channeléƒ½æœ‰å°ˆå±¬æ¨¡å‹ï¼Œæ€§èƒ½æœ€ä½³"
        elif best['channels_per_model'] >= 2.0:
            config_type = "é«˜æ•ˆå…±äº«"
            recommendation = "æ¨¡å‹å…±äº«æ•ˆç‡é«˜ï¼Œé©åˆé«˜ååé‡æ‡‰ç”¨"
        else:
            config_type = "å¹³è¡¡é…ç½®"
            recommendation = "FPSå’Œå»¶é²çš„å¹³è¡¡é»ï¼Œé©åˆå¤§å¤šæ•¸æ‡‰ç”¨"
        
        best['config_type'] = config_type
        best['recommendation'] = recommendation
        
        return best

    def _generate_optimization_report(self, results: List[Dict], best_config: Dict, 
                                    video_info: Dict, requested_channels: int) -> Dict:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "sdk_info": {
                "name": "Auto-Optimization Multi-Model Benchmark",
                "version": "1.0.0",
                "framework": "PyTorch + Ultralytics YOLO"
            },
            "test_configuration": {
                "video": video_info,
                "requested_channels": requested_channels,
                "model": self.model_name,
                "device": self.device,
                "img_size": self.img_size
            },
            "test_results": results,
            "best_configuration": best_config,
            "optimization_summary": {
                "total_tests": len(results),
                "best_model_count": best_config.get('model_count', 0),
                "best_avg_fps": best_config.get('avg_fps', 0),
                "best_total_fps": best_config.get('total_fps', 0),
                "best_latency": best_config.get('avg_latency', 0),
                "efficiency_score": best_config.get('efficiency_score', 0),
                "config_type": best_config.get('config_type', ''),
                "recommendation": best_config.get('recommendation', '')
            }
        }
        
        return report

    def _print_optimization_report(self, report: Dict):
        """é¡¯ç¤ºå„ªåŒ–å ±å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ è‡ªå‹•å„ªåŒ–çµæœå ±å‘Š")
        print(f"{'='*80}")
        
        # æœ€ä½³é…ç½®
        best = report['best_configuration']
        summary = report['optimization_summary']
        test_config = report['test_configuration']
        
        print(f"\nğŸ† æœ€ä½³é…ç½®:")
        print(f"  â€¢ æ¨¡å‹æ•¸é‡: {summary['best_model_count']}")
        print(f"  â€¢ é…ç½®é¡å‹: {summary['config_type']}")
        print(f"  â€¢ å¹³å‡FPS: {summary['best_avg_fps']:.2f}")
        print(f"  â€¢ ç¸½FPS: {summary['best_total_fps']:.2f}")
        print(f"  â€¢ å¹³å‡å»¶é²: {summary['best_latency']:.2f}ms")
        print(f"  â€¢ æ•ˆç‡åˆ†æ•¸: {summary['efficiency_score']:.2f}/100")
        print(f"  â€¢ å»ºè­°: {summary['recommendation']}")
        
        # æ‰€æœ‰æ¸¬è©¦çµæœ
        print(f"\nğŸ“Š æ‰€æœ‰æ¸¬è©¦çµæœ:")
        print(f"{'æ¨¡å‹æ•¸':<8} {'å¹³å‡FPS':<10} {'ç¸½FPS':<10} {'å»¶é²(ms)':<12} {'Avg CPU(%)':<12} {'Avg GPU(%)':<12} {'æ•ˆç‡åˆ†æ•¸':<10} {'é…ç½®é¡å‹'}")
        print(f"{'-'*95}")
        
        for result in report['test_results']:
            config_type = "ç†æƒ³" if result['is_ideal_config'] else "å…±äº«"
            resource_usage = result.get('resource_usage', {})
            avg_cpu = resource_usage.get('cpu', {}).get('average', 0.0)
            avg_gpu = resource_usage.get('gpu', {}).get('average', 0.0)
            
            print(f"{result['model_count']:<8} {result['avg_fps']:<10.2f} {result['total_fps']:<10.2f} "
                  f"{result['avg_latency']:<12.2f} {avg_cpu:<12.1f} {avg_gpu:<12.1f} "
                  f"{result['efficiency_score']:<10.2f} {config_type}")
        
        # ä½¿ç”¨å»ºè­°
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print(f"  â€¢ æœ€ä½³æŒ‡ä»¤: python fixed_channel_benchmark.py --video {test_config['video']['width']}x{test_config['video']['height']} --model {self.model_name} -n {test_config['requested_channels']} -m {summary['best_model_count']} -t 30")
        print(f"  â€¢ é æœŸæ€§èƒ½: æ¯å€‹Channelç´„{summary['best_avg_fps']:.1f} FPS")
        print(f"  â€¢ ç¸½ååé‡: {summary['best_total_fps']:.1f} frames/sec")

    def _fixed_channel_worker_thread(self,
                                   channel_id: int,
                                   model_id: int,
                                   video_path: str,
                                   stop_ts: float,
                                   metric: FixedChannelMetric,
                                   model: YOLO):
        """å›ºå®šChannelå·¥ä½œç·šç¨‹å‡½æ•¸ï¼ˆç”Ÿç”¢è€…-æ¶ˆè²»è€…æ¨¡å¼ï¼‰"""
        print(f"ğŸ”„ Channel {channel_id} é–‹å§‹å·¥ä½œ (ä½¿ç”¨Model {model_id})")
        
        frame_queue = queue.Queue(maxsize=10)
        
        # --- ç”Ÿç”¢è€…åŸ·è¡Œç·’ ---
        class ProducerThread(threading.Thread):
            # ... (ç”Ÿç”¢è€…ç¨‹å¼ç¢¼ä¿æŒä¸è®Š) ...
            def __init__(self, video_path, queue, stop_ts):
                super().__init__()
                self.daemon = True
                self.video_path = video_path
                self.queue = queue
                self.stop_ts = stop_ts
                self.read_times = []
                self.put_q_times = []
                self._stop_event = threading.Event()

            def run(self):
                cap = cv2.VideoCapture(self.video_path)
                if not cap.isOpened():
                    print(f"[Producer-{channel_id}] ç„¡æ³•æ‰“é–‹è¦–é »: {self.video_path}")
                    return
                
                try:
                    while time.time() < self.stop_ts and not self._stop_event.is_set():
                        t_read_start = perf_counter()
                        ret, frame = cap.read()
                        t_read_end = perf_counter()
                        self.read_times.append(t_read_end - t_read_start)

                        if not ret:
                            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                            continue
                        
                        t_put_start = perf_counter()
                        self.queue.put(frame)
                        t_put_end = perf_counter()
                        self.put_q_times.append(t_put_end - t_put_start)
                finally:
                    cap.release()
                    # ç™¼é€çµæŸä¿¡è™Ÿ
                    self.queue.put(None)

            def stop(self):
                self._stop_event.set()

        # --- æ¶ˆè²»è€…é‚è¼¯ ---
        consumer_get_q_times = []
        consumer_predict_times = []

        producer = ProducerThread(video_path, frame_queue, stop_ts)
        producer.start()
        
        try:
            while True:
                t_get_start = perf_counter()
                frame = frame_queue.get()
                t_get_end = perf_counter()
                consumer_get_q_times.append(t_get_end - t_get_start)

                if frame is None:
                    break # ç”Ÿç”¢è€…å·²çµæŸ

                # --- ğŸ‘‡ é€™è£¡æ˜¯ä¿®æ”¹é‡é» --- ğŸ‘‡
                # æ¥æ”¶ 2 å€‹è¿”å›å€¼ (detections, proc_time_s)
                detections, proc_time_s = self.predict_single_frame(model, frame)
                
                # å„²å­˜ç¸½ç‰†ä¸Šæ™‚é–“ (wall_s)
                consumer_predict_times.append({
                    'wall_s': proc_time_s
                })
                # --- ğŸ‘† ä¿®æ”¹çµæŸ --- ğŸ‘†
                
                metric.update(proc_time_s, len(detections))
                
        except Exception as e:
            print(f"[Channel {channel_id}] æ¶ˆè²»è€…éŒ¯èª¤: {e}")
        finally:
            producer.stop()
            producer.join()
            
            # å›å‚³å‰–ææ•¸æ“š
            metric.profiling_data = {
                'producer_read_times': producer.read_times,
                'producer_put_q_times': producer.put_q_times,
                'consumer_get_q_times': consumer_get_q_times,
                'consumer_predict_times': consumer_predict_times
            }

    def _get_video_info(self, video_path: str) -> Dict[str, Any]:
        """ç²å–è¦–é »ä¿¡æ¯"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return {'width': 0, 'height': 0, 'fps': 0, 'frame_count': 0}
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        cap.release()
        
        return {
            'width': width,
            'height': height,
            'fps': fps,
            'frame_count': frame_count
        }

    def _fixed_channel_monitor_progress(self, metrics: List[FixedChannelMetric], stop_ts: float):
        """å›ºå®šChannelé€²åº¦ç›£æ§"""
        last_emit_s = -1
        
        while time.time() < stop_ts:
            elapsed = int(stop_ts - time.time())
            now_s = int(time.time())
            
            if last_emit_s == -1 or (now_s - last_emit_s) >= 3:
                last_emit_s = now_s
                
                for metric in metrics:
                    fps = metric.get_fps()
                    latency = metric.get_latency_ms()
                    throughput = metric.get_throughput()
                    detections = metric.get_avg_detections()
                    
                    model_info = f"Model {metric.assigned_model_id}"
                    
                    print(
                        f"Channel {metric.channel_id} ({model_info}): fps={fps:.3f}, latency={latency:.2f}ms, "
                        f"detections={detections:.1f}"
                    )
                print("")
            
            time.sleep(0.5)

    def _generate_fixed_channel_report(self, 
                                     metrics: List[FixedChannelMetric], 
                                     config: Dict, 
                                     resource_stats: Dict, 
                                     micro_profiling: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆå›ºå®šChannelå ±å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "sdk_info": {
                "name": "Fixed Channel Multi-Model Parallel Benchmark",
                "version": "1.0.0",
                "framework": "PyTorch + Ultralytics YOLO"
            },
            "configuration": config,
            "summary": {
                "total_channels": len(metrics),
                "total_models": config['actual_models'],
                "channels_per_model": config['channels_per_model'],
                "total_frames": int(sum(m.num_frames for m in metrics)),
                "total_runtime": max(m.start_time for m in metrics) - min(m.start_time for m in metrics) if metrics else 0,
                "model_load_time": config.get('model_load_time', 0)
            },
            "performance_metrics": {},
            "hardware_analysis": {},
            "optimization_recommendations": {}
        }
        
        # --- ğŸ‘‡ é€™è£¡æ˜¯ä¿®æ­£é» #2 (å¤±èª¤ #2) --- ğŸ‘‡
        # æ€§èƒ½æŒ‡æ¨™ (è£œä¸Šå®Œæ•´çš„ performance_metrics å­—å…¸)
        if metrics:
            fps_values = [m.get_fps() for m in metrics if m.get_fps() > 0]
            latency_values = [m.get_latency_ms() for m in metrics if m.get_latency_ms() > 0]
            throughput_values = [m.get_throughput() for m in metrics if m.get_throughput() > 0]
            
            report["performance_metrics"] = {
                "fps": {
                    "average": float(np.mean(fps_values)) if fps_values else 0.0,
                    "min": float(np.min(fps_values)) if fps_values else 0.0,
                    "max": float(np.max(fps_values)) if fps_values else 0.0,
                    "per_channel": fps_values,
                    "total": float(np.sum(fps_values)) if fps_values else 0.0
                },
                "latency_ms": {
                    "average": float(np.mean(latency_values)) if latency_values else 0.0,
                    "min": float(np.min(latency_values)) if latency_values else 0.0,
                    "max": float(np.max(latency_values)) if latency_values else 0.0,
                    "per_channel": latency_values
                },
                "throughput": {
                    "total": float(np.sum(throughput_values)) if throughput_values else 0.0,
                    "per_channel": throughput_values
                },
                "resource_usage": resource_stats
            }
        # --- ğŸ‘† ä¿®æ­£çµæŸ --- ğŸ‘†

        # å¾®è§€æ€§èƒ½å‰–æ (åˆä½µ å®è§€å¯¦æ¸¬å€¼(B) å’Œ å¾®è§€ç†è«–å€¼(A))
        profiling_details = {}
        if metrics:
            for m in metrics:
                if m.profiling_data:
                    def _avg_ms(data, key=None):
                        if not data:
                            return 0.0
                        values = [d.get(key, 0) for d in data] if key else data
                        return (sum(values) / len(values)) * 1000 if values else 0.0

                    predict_times = m.profiling_data.get('consumer_predict_times', [])
                    
                    # 1. ç²å–å®è§€å¯¦æ¸¬æ•¸æ“š (Task B)
                    macro_data = {
                        "macro_producer_read_avg_ms": _avg_ms(m.profiling_data.get('producer_read_times', [])),
                        "macro_producer_put_q_avg_ms": _avg_ms(m.profiling_data.get('producer_put_q_times', [])),
                        "macro_consumer_get_q_avg_ms": _avg_ms(m.profiling_data.get('consumer_get_q_times', [])),
                        "macro_consumer_wall_avg_ms": _avg_ms(predict_times, key='wall_s'), # é€™æ˜¯ç¸½å»¶é²
                    }
                    
                    # 2. å­˜å„²å®è§€æ•¸æ“š
                    profiling_details[f"channel_{m.channel_id}"] = macro_data
                    
                    # 3. ä½µå…¥å¾®è§€ç†è«–æ•¸æ“š (Task A)
                    # (micro_profiling æ˜¯å¾ benchmark_video_fixed_channels å‚³å…¥çš„)
                    profiling_details[f"channel_{m.channel_id}"].update(micro_profiling)

        
        # å°‡å‰–ææ•¸æ“šåŠ å…¥åˆ° performance_metrics ä¸­
        if profiling_details:
            report["performance_metrics"]["profiling_details"] = profiling_details
        
        # ç¡¬é«”åˆ†æ
        report["hardware_analysis"] = {
            "hardware_specs": self.hardware_specs,
            "channel_allocation": {
                "requested_channels": config['requested_channels'],
                "actual_models": config['actual_models'],
                "channels_per_model": config['channels_per_model'],
                "allocation_efficiency": min(100.0, config['actual_models'] / config['requested_channels'] * 100),
                "is_ideal_config": config['actual_models'] >= config['requested_channels']
            },
            "memory_utilization": {
                # "estimated_model_memory": 0, # å·²æ£„ç”¨
                "total_used_memory": config.get('load_info', {}).get('total_memory_usage', 0),
                "available_memory": self.hardware_specs.get('gpu_memory_gb', 0) if self.device != 'cpu' else self.hardware_specs.get('total_memory_gb', 0)
            }
        }
        
        # å„ªåŒ–å»ºè­°
        recommendations = []
        
        if config['actual_models'] >= config['requested_channels']:
            recommendations.append("âœ… ç†æƒ³é…ç½®ï¼šæ¯å€‹Channeléƒ½æœ‰å°ˆå±¬æ¨¡å‹")
            recommendations.append("âœ… æ€§èƒ½æœ€ä½³ï¼šç„¡æ¨¡å‹å…±äº«ï¼Œç„¡è³‡æºç«¶çˆ­")
        else:
            recommendations.append(f"âš ï¸ æ¨¡å‹å…±äº«ï¼šæ¯å€‹æ¨¡å‹è™•ç† {config['channels_per_model']:.1f} å€‹Channel")
            recommendations.append(f"âš ï¸ ç¡¬é«”é™åˆ¶ï¼šåªèƒ½è¼‰å…¥ {config['actual_models']}/{config['requested_channels']} å€‹æ¨¡å‹")
            
            if config['requested_channels'] > 8:
                recommendations.append("ğŸ’¡ åŸå› ï¼šGPUé‹ç®—ç“¶é ¸ï¼Œé¿å…éå¤šæ¨¡å‹åŒæ™‚é‹è¡Œ")
                recommendations.append("ğŸ’¡ å»ºè­°ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹ (yolov8n) ä»¥è¼‰å…¥æ›´å¤šå¯¦ä¾‹")
                recommendations.append("ğŸ’¡ å»ºè­°ï¼šè€ƒæ…®ä½¿ç”¨æ‰¹æ¬¡è™•ç†ä¾†æå‡æ•ˆç‡")
            else:
                if self.device != 'cpu':
                    recommendations.append("ğŸ’¡ å»ºè­°ï¼šå‡ç´šGPUè¨˜æ†¶é«”ä»¥æ”¯æ´æ›´å¤šæ¨¡å‹")
                    recommendations.append("ğŸ’¡ å»ºè­°ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹ (yolov8n) ä»¥è¼‰å…¥æ›´å¤šå¯¦ä¾‹")
                else:
                    recommendations.append("ğŸ’¡ å»ºè­°ï¼šå¢åŠ ç³»çµ±è¨˜æ†¶é«”æˆ–ä½¿ç”¨GPUåŠ é€Ÿ")
        
        report["optimization_recommendations"] = recommendations
        
        return report

    def _print_fixed_channel_report(self, report: Dict[str, Any]):
        """æ‰“å°å›ºå®šChannelå ±å‘Š"""
        print("\n" + "="*80)
        print("ğŸš€ Innodisk Computer Vision Benchmark æ¸¬è©¦å ±å‘Š v1.0")
        print("="*80)
        
        # æ¸¬è©¦é…ç½®
        config = report["configuration"]
        print(f"\nğŸ“Š æ¸¬è©¦é…ç½®:")
        print(f"  â€¢ æ¨¡å‹: {config['model']}")
        print(f"  â€¢ è¦–é »: {config['video']}")
        print(f"  â€¢ è«‹æ±‚Channelæ•¸: {config['requested_channels']}")
        print(f"  â€¢ å¯¦éš›æ¨¡å‹æ•¸: {config['actual_models']}")
        print(f"  â€¢ æ¯æ¨¡å‹è™•ç†Channelæ•¸: {config['channels_per_model']:.1f}")
        if config.get('fixed_models') is not None:
            print(f"  â€¢ å›ºå®šæ¨¡å‹æ•¸é‡: {config['fixed_models']} (ç”¨æˆ¶æŒ‡å®š)")
        print(f"  â€¢ æ¨¡å‹è¼‰å…¥æ™‚é–“: {config['model_load_time']:.3f}ç§’")
        print(f"  â€¢ ç¸½åŸ·è¡Œæ™‚é–“: {config['total_execution_time']:.3f}ç§’")
        print(f"  â€¢ æ¨¡å‹è¼¸å…¥å°ºå¯¸: {config['img_size']}x{config['img_size']}")
        print(f"  â€¢ è¦–é »è§£æåº¦: {config['video_resolution']}")
        print(f"  â€¢ è¦–é »FPS: {config['video_fps']:.2f}")
        print(f"  â€¢ ç½®ä¿¡åº¦é–¾å€¼: {config['conf']}")
        print(f"  â€¢ IoUé–¾å€¼: {config['iou']}")
        print(f"  â€¢ æ¸¬è©¦æŒçºŒæ™‚é–“: {config['seconds']}ç§’")
        print(f"  â€¢ è¨­å‚™: {config['device']}")
        
        # ç¡¬é«”è¦æ ¼
        hw_specs = config['hardware_specs']
        print(f"\nğŸ’» ç¡¬é«”è¦æ ¼:")
        print(f"  â€¢ CPUæ ¸å¿ƒæ•¸: {hw_specs['cpu_cores']}")
        print(f"  â€¢ CPUç·šç¨‹æ•¸: {hw_specs['cpu_threads']}")
        print(f"  â€¢ ç³»çµ±è¨˜æ†¶é«”: {hw_specs['total_memory_gb']:.1f} GB")
        if hw_specs['cuda_available']:
            print(f"  â€¢ GPUæ•¸é‡: {hw_specs['gpu_count']}")
            if hw_specs['gpu_count'] > 1:
                # å¤šGPUç’°å¢ƒï¼šåˆ†åˆ¥é¡¯ç¤ºæ¯å€‹GPU
                for gpu in hw_specs['gpus']:
                    print(f"  â€¢ GPU {gpu['id']}: {gpu['name']} ({gpu['memory_gb']:.1f} GB, Compute {gpu['compute_capability']})")
            else:
                # å–®GPUç’°å¢ƒï¼šä¿æŒåŸæœ‰æ ¼å¼
                print(f"  â€¢ GPUè¨˜æ†¶é«”: {hw_specs['gpu_memory_gb']:.1f} GB")
                print(f"  â€¢ GPUåç¨±: {hw_specs['gpu_name']}")
        
        # æ€§èƒ½æŒ‡æ¨™
        if "performance_metrics" in report and report["performance_metrics"]:
            perf = report["performance_metrics"]
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ¨™:")
            print(f"  â€¢ å¹³å‡æ¯Channel FPS: {perf['fps']['average']:.2f}")
            print(f"  â€¢ FPSç¯„åœ: {perf['fps']['min']:.2f} - {perf['fps']['max']:.2f}")
            print(f"  â€¢ ç¸½FPS (æ‰€æœ‰Channelåˆè¨ˆ): {perf['fps']['total']:.2f}")
            print(f"  â€¢ å¹³å‡å»¶é²: {perf['latency_ms']['average']:.2f}ms")
            print(f"  â€¢ å»¶é²ç¯„åœ: {perf['latency_ms']['min']:.2f} - {perf['latency_ms']['max']:.2f}ms")
            
            # è³‡æºä½¿ç”¨ç‡çµ±è¨ˆ
            if "resource_usage" in perf:
                res = perf["resource_usage"]
                print(f"\nğŸ’» è³‡æºä½¿ç”¨ç‡:")
                print(f"  â€¢ CPUä½¿ç”¨ç‡: å¹³å‡ {res['cpu']['average']:.1f}% (ç¯„åœ: {res['cpu']['min']:.1f}% - {res['cpu']['max']:.1f}%)")
                print(f"  â€¢ è¨˜æ†¶é«”ä½¿ç”¨ç‡: å¹³å‡ {res['memory']['average']:.1f}% (ç¯„åœ: {res['memory']['min']:.1f}% - {res['memory']['max']:.1f}%)")
                print(f"  â€¢ GPUä½¿ç”¨ç‡: å¹³å‡ {res['gpu']['average']:.1f}% (ç¯„åœ: {res['gpu']['min']:.1f}% - {res['gpu']['max']:.1f}%)")
        
        # ç¡¬é«”åˆ†æ
        hw_analysis = report["hardware_analysis"]
        print(f"\nğŸ” ç¡¬é«”åˆ†æ:")
        print(f"  â€¢ Channelåˆ†é…: {hw_analysis['channel_allocation']['requested_channels']} â†’ {hw_analysis['channel_allocation']['actual_models']} æ¨¡å‹")
        print(f"  â€¢ æ¯æ¨¡å‹è™•ç†: {hw_analysis['channel_allocation']['channels_per_model']:.1f} å€‹Channel")
        print(f"  â€¢ åˆ†é…æ•ˆç‡: {hw_analysis['channel_allocation']['allocation_efficiency']:.1f}%")
        
        if hw_analysis['channel_allocation']['is_ideal_config']:
            print(f"  â€¢ é…ç½®ç‹€æ…‹: âœ… ç†æƒ³é…ç½® (æ¯å€‹Channeléƒ½æœ‰å°ˆå±¬æ¨¡å‹)")
        else:
            print(f"  â€¢ é…ç½®ç‹€æ…‹: âš ï¸ æ¨¡å‹å…±äº« (å¤šå€‹Channelå…±äº«æ¨¡å‹)")
        
        # print(f"  â€¢ ä¼°ç®—æ¨¡å‹è¨˜æ†¶é«”: {hw_analysis['memory_utilization']['estimated_model_memory']:.1f} GB") # å·²æ£„ç”¨
        print(f"  â€¢ ç¸½ä½¿ç”¨è¨˜æ†¶é«”: {hw_analysis['memory_utilization']['total_used_memory']:.1f} GB")
        print(f"  â€¢ å¯ç”¨è¨˜æ†¶é«”: {hw_analysis['memory_utilization']['available_memory']:.1f} GB")
        
        # å„ªåŒ–å»ºè­°
        if report["optimization_recommendations"]:
            print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            for i, recommendation in enumerate(report["optimization_recommendations"], 1):
                print(f"  {i}. {recommendation}")
        
        # æ•ˆç‡åˆ†æ•¸è¨ˆç®—èªªæ˜
        print(f"\nğŸ“Š æ•ˆç‡åˆ†æ•¸è¨ˆç®—èªªæ˜:")
        print(f"  æ•ˆç‡åˆ†æ•¸æ˜¯ä¸€å€‹ç¶œåˆè©•åˆ†ç³»çµ± (ç¸½åˆ†100åˆ†)ï¼Œç”¨ä¾†è©•ä¼°ä¸åŒæ¨¡å‹é…ç½®çš„æ•´é«”æ€§èƒ½è¡¨ç¾ï¼š")
        print(f"  â€¢ FPSåˆ†æ•¸ (40%æ¬Šé‡): ä»¥30 FPSç‚ºæ»¿åˆ†ï¼Œè¨ˆç®—å…¬å¼: min(100, (å¹³å‡FPS/30) Ã— 100)")
        print(f"  â€¢ å»¶é²åˆ†æ•¸ (30%æ¬Šé‡): ä»¥100msç‚ºåŸºæº–ï¼Œå»¶é²è¶Šä½åˆ†æ•¸è¶Šé«˜ï¼Œè¨ˆç®—å…¬å¼: max(0, 100 - (å¹³å‡å»¶é²/100) Ã— 100)")
        print(f"  â€¢ æ•ˆç‡åˆ†æ•¸ (30%æ¬Šé‡): ç†æƒ³é…ç½®(æ¯Channelå°ˆå±¬æ¨¡å‹)ç‚º100åˆ†ï¼Œå…±äº«æ¨¡å‹ç‚º channels_per_model Ã— 100")
        print(f"  â€¢ ç¸½åˆ†è¨ˆç®—: FPSåˆ†æ•¸Ã—0.4 + å»¶é²åˆ†æ•¸Ã—0.3 + æ•ˆç‡åˆ†æ•¸Ã—0.3")
        print(f"  â€¢ åˆ†æ•¸æ„ç¾©: 0-30åˆ†(éœ€å„ªåŒ–) | 30-60åˆ†(å¯æ¥å—) | 60-80åˆ†(è‰¯å¥½) | 80-100åˆ†(å„ªç§€)")
        
        print("\n" + "="*80)

    def _save_report(self, report: Dict[str, Any], output_file: str):
        """ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="å›ºå®šChannelæ•¸é‡çš„å¤šæ¨¡å‹ä¸¦è¡ŒåŸºæº–æ¸¬è©¦å·¥å…·")
    parser.add_argument("--video", type=str, required=True, help="è¦–é »æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--model", type=str, default="yolov8n.pt", help="YOLO æ¨¡å‹åç¨±æˆ–è·¯å¾‘")
    parser.add_argument("-n", "--channels", type=int, default=4, help="å›ºå®šçš„ä¸¦è¡ŒChannelæ•¸ï¼ˆä¸æœƒæ”¹è®Šï¼‰")
    parser.add_argument("-m", "--models", type=int, help="å›ºå®šè¼‰å…¥çš„æ¨¡å‹æ•¸é‡ï¼ˆè¦†è“‹è‡ªå‹•è¨ˆç®—ï¼‰")
    parser.add_argument("--auto-optimize", action="store_true", help="è‡ªå‹•æ¸¬è©¦å¾1åˆ°Nå€‹æ¨¡å‹æ•¸é‡ï¼Œæ‰¾åˆ°æœ€ä½³å¹³è¡¡é»")
    parser.add_argument("-t", "--seconds", type=int, default=60, help="æ¸¬è©¦æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--img-size", type=int, default=640, help="æ¨¡å‹è¼¸å…¥å°ºå¯¸")
    parser.add_argument("--conf", type=float, default=0.25, help="ç½®ä¿¡åº¦é–¾å€¼")
    parser.add_argument("--iou", type=float, default=0.5, help="IoU é–¾å€¼")
    parser.add_argument("--device", type=str, default="cuda", help="è¨­å‚™é…ç½® (auto, cpu, cuda)")
    parser.add_argument("--output", type=str, help="è¼¸å‡ºå ±å‘Šæ–‡ä»¶è·¯å¾‘ (å–®æ¬¡æ¸¬è©¦) æˆ–å ±å‘Šç›®éŒ„ (è‡ªå‹•å„ªåŒ–)")
    
    args = parser.parse_args()
    
    # å°‡ output åƒæ•¸ä½œç‚º output_dir å‚³éçµ¦è‡ªå‹•å„ªåŒ–
    if args.output:
        args.output_dir = args.output
    else:
        args.output_dir = "reports"

    try:
        # å‰µå»ºå›ºå®šChannelåŸºæº–æ¸¬è©¦å™¨
        benchmark = FixedChannelBenchmark(
            model_name=args.model,
            device=args.device,
            img_size=args.img_size,
            conf_threshold=args.conf,
            iou_threshold=args.iou
        )
        
        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        if args.auto_optimize:
            # è‡ªå‹•å„ªåŒ–æ¨¡å¼ï¼šæ¸¬è©¦å¾1åˆ°Nå€‹æ¨¡å‹æ•¸é‡
            report = benchmark.run_auto_optimization(args)
        else:
            # å–®æ¬¡æ¸¬è©¦æ¨¡å¼
            report = benchmark.benchmark_video_fixed_channels(
                video_path=args.video,
                duration_seconds=args.seconds,
                requested_channels=args.channels,
                fixed_models=args.models,
                output_file=args.output
            )
        
        print("\nâœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
